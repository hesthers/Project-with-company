{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FM modeling (10).ipynb",
      "provenance": [],
      "mount_file_id": "11iVVSUKJHN9ExvxlMbiXvRZ2S-CMP9in",
      "authorship_tag": "ABX9TyOnSvcbcdLKw4GpC1cNcdVC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qzDyuZ5ICh2B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from itertools import repeat\n",
        "from time import perf_counter\n",
        "from tensorflow.keras.metrics import BinaryAccuracy, AUC\n",
        "from tensorflow.python.client import device_lib\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
        "pd.set_option('display.max_columns', None)\n",
        "tf.keras.backend.set_floatx('float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_SIZE = 10\n",
        "cols = {'obj': [],\n",
        "        'cat': [],\n",
        "       'cont': []\n",
        "        }\n",
        "def data_split():\n",
        "    file_name = 'final_data_v2.csv'\n",
        "    file_path = os.getcwd()+'/drive/MyDrive/Colab Notebooks/' #[:-16]는 본인 경로에 맞게 있어도 되고 없어도 됨.\n",
        "    df = pd.read_csv(file_path+file_name, encoding='utf-8')\n",
        "    df.rename(columns={'category_id_1':'category1'}, inplace=True)\n",
        "\n",
        "    # 데이터 유형별 분류하기\n",
        "    for dt_idx, dt_val in zip(df.dtypes.index, df.dtypes.values):\n",
        "        # if 'category' in dt_idx:\n",
        "        #     df[['category1']] = LabelEncoder().fit_transform(df[['category1']])\n",
        "        #     cols['cat'].append('category1')\n",
        "\n",
        "        if dt_val == 'object':\n",
        "            if ('id' in dt_idx) | ('time' in dt_idx) | ('name' in dt_idx) | ('keyword' in dt_idx) |('url' in dt_idx):\n",
        "                df.drop(columns = dt_idx, axis=1, inplace=True)\n",
        "            else:\n",
        "                cols['obj'].append(dt_idx)\n",
        "\n",
        "        else:\n",
        "            if ('id' in dt_idx) | ('time' in dt_idx):\n",
        "                df.drop(columns = dt_idx, axis=1, inplace=True)\n",
        "            else:\n",
        "                if len(df[dt_idx].value_counts()) <= 30: #연속형 데이터 중 30개 내의 범주로 나눌 수 있는 데이터 = category로 구분.\n",
        "                    cols['cat'].append(dt_idx)\n",
        "                else:\n",
        "                    if ('hour' in dt_idx) | ('group' in dt_idx):\n",
        "                        pass\n",
        "                    else:\n",
        "                        cols['cont'].append(dt_idx) \n",
        "\n",
        "    return cols\n",
        "\n",
        "def reorganization(df):\n",
        "  data = pd.DataFrame()\n",
        "  cols = data_split()\n",
        "  for k, v in cols.items():\n",
        "    if k == 'obj':\n",
        "      data = pd.concat([data, df[v]], axis=1)\n",
        "    elif k == 'cont':\n",
        "      data = pd.concat([data, df[v]], axis=1)\n",
        "    else:\n",
        "      data = pd.concat([data, df[v]], axis=1)\n",
        "\n",
        "  return data\n",
        "\n",
        "def preprocessing():\n",
        "    file_name = 'final_data_v2.csv'\n",
        "    file_path = os.getcwd()+'/drive/MyDrive/Colab Notebooks/' \n",
        "    df = pd.read_csv(file_path+file_name, encoding='utf-8')\n",
        "    df.rename(columns={'category_id_1':'category1'}, inplace=True)\n",
        "    # 데이터 유형별 분류하기\n",
        "    data = reorganization(df)\n",
        "    # cols = data_split()\n",
        "    modified_df = pd.DataFrame()\n",
        "    vec_dict = {idx: [] for idx in range(len(data.columns))}\n",
        "    feature_index = []\n",
        "\n",
        "    for i, c in enumerate(data.columns):\n",
        "        if c in cols['obj']:\n",
        "            obj_data = pd.get_dummies(data[c], prefix=c, prefix_sep = \"/\")\n",
        "            modified_df = pd.concat([modified_df, obj_data], axis=1)\n",
        "            vec_dict[i] = list(obj_data.columns)\n",
        "            feature_index.extend(repeat(i, obj_data.shape[1]))\n",
        "\n",
        "        elif c in cols['cat']:  # click_label 컬럼 = y 변수로 사용\n",
        "            if 'click' in c:\n",
        "                pass\n",
        "            else:\n",
        "                cat_data = pd.get_dummies(data[c], prefix=c, prefix_sep = \"/\")\n",
        "                vec_dict[i] = list(cat_data.columns)\n",
        "                feature_index.extend(repeat(i, cat_data.shape[1]))\n",
        "                modified_df = pd.concat([modified_df, cat_data], axis=1)\n",
        "        else:\n",
        "            scaled_num_data = MinMaxScaler().fit_transform(df[[c]])\n",
        "            scaled_num_data = pd.DataFrame(scaled_num_data, columns = [c])\n",
        "            modified_df = pd.concat([modified_df,scaled_num_data], axis=1)\n",
        "            vec_dict[i] = list(scaled_num_data.columns)\n",
        "            feature_index.extend(repeat(i, scaled_num_data.shape[1]))\n",
        "\n",
        "    print('---- Data info ----')\n",
        "    # print(cols)\n",
        "    print('Data Frame shape: {}'.format(modified_df.shape))\n",
        "    print('# of Feature: {}'.format(len(feature_index)))\n",
        "    print(f'# of Field: {len(vec_dict)}')\n",
        "    # print(f'Modified DF columns: {modified_df.columns}')\n",
        "    # print(vec_dict)\n",
        "    return vec_dict, feature_index, modified_df\n",
        "\n",
        "# 데이터 7:3으로 나누기 (혹은 8:2)\n",
        "def split_data():\n",
        "    file_name = 'final_data_v2.csv'\n",
        "    file_path = os.getcwd()+'/drive/MyDrive/Colab Notebooks/' \n",
        "    df = pd.read_csv(file_path+file_name, encoding='utf-8')\n",
        "    df.rename(columns={'category_id_1':'category1'}, inplace=True)\n",
        "\n",
        "    vec_dict, feature_index, modified_df = preprocessing()\n",
        "\n",
        "    X = modified_df#.astype('float')\n",
        "    y = df['click_label']\n",
        "\n",
        "    print(f\"X features' name (10): {X.columns.to_list()[:10]}\")\n",
        "    print(f\"y feature's name: {y.name}\")\n",
        "    print()\n",
        "    \n",
        "    oversample = SMOTE(random_state=2022) # 불균형 데이터 셋인 번개장터 데이터 셋 불균형 문제 완화\n",
        "    X, y = oversample.fit_resample(X, y)\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 2022, stratify = y) #y 비율에 따른 층화추출 및 데이터를 7:3의 비율로 나누기\n",
        "    \n",
        "    print(f\"# of train_data's rows: {X_train.shape[0]} \\n# of test_data's rows: {X_test.shape[0]}\")\n",
        "    print(f'train:test ratio = {round(X_train.shape[0]/(X_train.shape[0]+ X_test.shape[0]),2)}:{round(X_test.shape[0]/(X_train.shape[0]+ X_test.shape[0]), 2)}')\n",
        "\n",
        "    # tf.data.Dataset.from_tensor_slices 함수: tf.data.Dataset 를 생성하는 함수로 입력된 텐서로부터 slices를 생성.\n",
        "    # shuffle 함수는 고정된 버퍼 크기로 데이터를 섞는데, 데이터가 완전히 랜덤적으로 뒤섞기 위해서는 입력된 데이터 크기보다 큰 수를 입력.\n",
        "    # tf.cast 함수: 뒤에 나온 형으로 값을 변환\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices( \n",
        "              (tf.cast(X_train.values, tf.float32), tf.cast(y_train, tf.float32))\n",
        "            ).shuffle(600000).batch(BATCH_SIZE) \n",
        "    \n",
        "    test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "              (tf.cast(X_test.values, tf.float32), tf.cast(y_test, tf.float32))\n",
        "            ).shuffle(300000).batch(BATCH_SIZE)\n",
        "    \n",
        "    print(f'Current Batch Size: {BATCH_SIZE}')\n",
        "    print(f'train_ds: {train_ds}')\n",
        "    print(f'test_ds: {test_ds}')\n",
        "    return train_ds, test_ds, vec_dict, feature_index"
      ],
      "metadata": {
        "id": "qCux7Kr4Ck96"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FM_layer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_feature, grouped_field, embedding_size, feature_index):\n",
        "        super(FM_layer, self).__init__()\n",
        "        self.embedding_size = embedding_size    # k: 임베딩 벡터의 차원(크기)\n",
        "        self.num_feature = num_feature          # f: 원래 feature 개수\n",
        "        self.grouped_field = grouped_field      # m: grouped field 개수\n",
        "        self.feature_index = feature_index      # 인코딩된 X의 칼럼들이 본래 어디 소속이었는지\n",
        "\n",
        "        # Parameters of FM Layer\n",
        "        # w: capture 1st order interactions (linear - num_feature 만큼의 크기를 가진 벡터)\n",
        "        # V: capture 2nd order interactions\n",
        "        # tf.Variable: 모델링에서 weight나 bias와 같은 변수 값을 초기화하는 훈련가능한 변수\n",
        "        self.w = tf.Variable(tf.random.normal(shape=[num_feature], mean=0.0, stddev=1.0)\n",
        "                           , name='w'\n",
        "                            )\n",
        "        self.V = tf.Variable(tf.random.normal(shape=(grouped_field, embedding_size), \n",
        "                                              mean=0.0, stddev=0.01)\n",
        "                           , name='V'\n",
        "                            )\n",
        "\n",
        "    def call(self, inputs):\n",
        "          # print(f\"input vector's shape: {inputs.shape}\") #input vector's shape: (256, 101)\n",
        "        x_batch = tf.reshape(tf.expand_dims(inputs, axis=-1), [-1, self.num_feature, 1]) #-1: 가로 vector로 생성, self.feature_index 만큼의 행, 1열\n",
        "        # print(f\"X's Batch Shape: {x_batch.shape}\") #X's Batch Shape: (256, 101, 1)                 \n",
        "        \n",
        "        # Parameter V를 feature_index에 맞게 복사하여 num_feature에 맞게 늘림 (field 수만큼 embedding)\n",
        "        embeds = tf.nn.embedding_lookup(params=self.V, ids=self.feature_index)\n",
        "        # print(f\"Embedding Layer Shape: {embeds.shape}\") #Embedding Layer Shape: (101, 10)\n",
        "        \n",
        "        # Deep Component에서 쓸 Input\n",
        "        # (batch_size, num_feature, embedding_size)\n",
        "        # order-2 layer (inner product of respective feature latent vectors)\n",
        "        vector_inputs = tf.math.multiply(x_batch, embeds) \n",
        "        # print(f'Input Layer Shape: {vector_inputs.shape}') #Input Layer Shape: (256, 101, 10)\n",
        "\n",
        "        # (batch_size, ) -> order-1 layer (linear interactions among features)\n",
        "        linear_terms = tf.reduce_sum(tf.math.multiply(self.w, inputs), axis=1, keepdims=False)\n",
        "\n",
        "        # (batch_size, ) -> order-2 features (inner product units)\n",
        "        # tf.math.pow(tf.matmul(inputs, self.V), 2) - tf.matmul(tf.math.pow(inputs, 2), tf.math.pow(self.V, 2)\n",
        "        # tf.matmul: 행렬의 곱셈\n",
        "        interactions = 0.5 * tf.subtract(\n",
        "            tf.square(tf.reduce_sum(vector_inputs, [1, 2])),\n",
        "            tf.reduce_sum(tf.square(vector_inputs), [1, 2])\n",
        "        )\n",
        "        \n",
        "        # sigmoid function for CTR prediction\n",
        "        linear_terms = tf.reshape(linear_terms, [-1, 1]) #벡터 -> tensor화된 벡터로 변환\n",
        "        interactions = tf.reshape(interactions, [-1, 1])\n",
        "        y_fm = tf.concat([linear_terms, interactions], 1)\n",
        "\n",
        "        return y_fm, vector_inputs"
      ],
      "metadata": {
        "id": "DMNP-jC-CwuU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepFM(tf.keras.Model):\n",
        "    def __init__(self, num_feature, grouped_field, embedding_size, feature_index):\n",
        "        super(DeepFM, self).__init__()\n",
        "        self.embedding_size = embedding_size    # k: 임베딩 벡터의 차원(크기)\n",
        "        self.num_feature = num_feature          # f: 원래 feature 개수\n",
        "        self.grouped_field = grouped_field      # m: grouped field 개수\n",
        "        self.feature_index = feature_index      # 인코딩된 X의 칼럼들이 본래 어디 소속이었는지 (칼럼 인덱스)\n",
        "\n",
        "        self.fm_layer = FM_layer(num_feature, grouped_field, embedding_size, feature_index)\n",
        "        \n",
        "        self.hidden_layer1 = tf.keras.layers.Dense(units=64, activation='relu') #tf.keras.layers.Dense(레이어 사이즈, 활성화 함수): 인공신경망 구조를 구현시켜주는 함수\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate=0.4)\n",
        "        self.hidden_layer2 = tf.keras.layers.Dense(units=32, activation='relu')\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate=0.2)\n",
        "        self.hidden_layer3 = tf.keras.layers.Dense(units=16, activation='relu')\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate=0.1)\n",
        "        self.hidden_layer4 = tf.keras.layers.Dense(units=2, activation='relu')\n",
        "\n",
        "        self.output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"DeepFM Model: # Field: {}, # Feature: {}, Embedding: {}\".format(self.grouped_field, self.num_feature, self.embedding_size)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        # 1) FM Component: (num_batch, 2)\n",
        "        y_fm, vector_inputs = self.fm_layer(inputs)\n",
        "        # print(f'Sigmoid Function shape: {y_fm.shape}') #Sigmoid Function shape: (256, 2)\n",
        "        # print(f'input vector shape: {vector_inputs.shape}') #input vector shape: (256, 101, 10)\n",
        "\n",
        "        # retrieve Dense Vectors: (num_batch, num_feature*embedding_size)\n",
        "        vector_inputs = tf.reshape(vector_inputs, [-1, self.num_feature*self.embedding_size])\n",
        "\n",
        "        # 2) Deep Component\n",
        "        y_deep = self.hidden_layer1(vector_inputs)\n",
        "        y_deep = self.dropout1(y_deep)\n",
        "        y_deep = self.hidden_layer2(y_deep)\n",
        "        y_deep = self.dropout2(y_deep)\n",
        "        y_deep = self.hidden_layer3(y_deep)\n",
        "        y_deep = self.dropout3(y_deep)\n",
        "        y_deep = self.hidden_layer4(y_deep)\n",
        "\n",
        "        # Concatenation\n",
        "        y_pred = tf.concat([y_fm, y_deep], 1)\n",
        "        y_pred = self.output_layer(y_pred)\n",
        "        y_pred = tf.reshape(y_pred, [-1, ])\n",
        "        \n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "omAYlaETCz2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_per_batch(model, x, y, opt, train_acc, train_auc):\n",
        "  with tf.GradientTape() as gt: #GradientTapes can be nested to compute higher-order derivatives. (자동으로 미분 실행.)\n",
        "      y_pred = model(x)\n",
        "      loss = tf.keras.losses.binary_crossentropy(from_logits=False, y_true=y, y_pred=y_pred)\n",
        "\n",
        "  grads = gt.gradient(target=loss, sources=model.trainable_variables)\n",
        "\n",
        "  # apply_gradients()를 통해 processed gradients를 적용함\n",
        "  opt.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  # accuracy & auc 성능\n",
        "  train_acc.update_state(y, y_pred)\n",
        "  train_auc.update_state(y, y_pred)\n",
        "\n",
        "  return loss\n",
        "\n",
        "# 반복 학습 함수\n",
        "def training(epoch):\n",
        "    # X_train, X_test, y_train, y_test = split_data()\n",
        "    train_ds, test_ds, vec_dict, feature_index = split_data()\n",
        "    \n",
        "    model = DeepFM(embedding_size= EMBEDDING_SIZE, \n",
        "                   num_feature=len(feature_index),\n",
        "                   grouped_field=len(vec_dict), \n",
        "                   feature_index=feature_index)\n",
        "    '''\n",
        "        self.embedding_size = embedding_size    # k: 임베딩 벡터의 차원(크기)\n",
        "        self.num_feature = num_feature          # f: 원래 feature 개수 (shape[1]에 해당)\n",
        "        self.grouped_field = grouped_field              # m: grouped field 개수\n",
        "        self.field_index = field_index          # 인코딩된 X의 칼럼들이 본래 어디 소속이었는지 (칼럼 인덱스)\n",
        "        \n",
        "        print('X shape: {}'.format(X_modified.shape))  \n",
        "        print('# of Feature: {}'.format(len(field_index)))\n",
        "        print('# of Field: {}'.format(len(field_dict)))\n",
        "    '''\n",
        "    # Momentum 장점 + AdaGrad 장점 = Adam (모멘텀 방식보다 좌우 흔들림이 덜 함. 좌우흔들림이 덜 함.)\n",
        "    # 추가로 논문에 나온 대로 FTRL 기법 사용\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.01) #learning_rate은 별도로 조절\n",
        "    ''' \n",
        "    var1 = tf.Variable(10.0)\n",
        "    loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1\n",
        "    step_count = opt.minimize(loss, [var1]).numpy()\n",
        "    # The first step is `-learning_rate*sign(grad)`\n",
        "    var1.numpy(): 9.9\n",
        "    '''\n",
        "  \n",
        "    start = perf_counter()\n",
        "    print(\"Start Training: Batch Size: {}, Embedding Size: {}\".format(BATCH_SIZE, EMBEDDING_SIZE))\n",
        "\n",
        "    for i in range(epoch):\n",
        "      train_acc = BinaryAccuracy(threshold=0.5) #threshold값도 조절 (0.4~0.6 사이 값)\n",
        "      train_auc = AUC()\n",
        "      loss_history = []\n",
        "\n",
        "      for x, y in train_ds:\n",
        "          loss = training_per_batch(model, x, y, opt, train_acc, train_auc)\n",
        "          loss_history.append(loss)\n",
        "      \n",
        "      # if i % 10 == 9:\n",
        "      print(\"Epoch {}: 누적 Loss: {:.4f}, Acc: {:.4f}, AUC: {:.4f}\".format(i+1, np.mean(loss_history), train_acc.result().numpy(), train_auc.result().numpy()))\n",
        "      # else:\n",
        "      #   pass\n",
        "\n",
        "    print(\"End of Training\")\n",
        "    test_acc = BinaryAccuracy(threshold=0.5)\n",
        "    test_auc = AUC()\n",
        "    for x, y in test_ds:\n",
        "        y_pred = model(x)\n",
        "        test_acc.update_state(y, y_pred)\n",
        "        test_auc.update_state(y, y_pred)\n",
        "    \n",
        "    print(\"테스트 ACC: {:.4f}, AUC: {:.4f}\".format(test_acc.result().numpy(), test_auc.result().numpy()))\n",
        "    print(\"Batch Size: {}, Embedding Size: {}\".format(BATCH_SIZE, EMBEDDING_SIZE))\n",
        "    print(f\"걸린 시간: {round((perf_counter() - start)//60)}분 {round((perf_counter() - start)%60)}초\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "yBJ4H3JeC3DZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 10번 후 확인해보기 (relu 활성화 함수 사용)\n",
        "if __name__ == '__main__':\n",
        "  training(10)"
      ],
      "metadata": {
        "id": "C8cQIq9MDBHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8d3c1e1-7388-46c3-d9df-1c9854283882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Data info ----\n",
            "Data Frame shape: (842463, 101)\n",
            "# of Feature: 101\n",
            "# of Field: 27\n",
            "X features' name (10): ['user_gender/F', 'user_gender/M', 'imp_hour/0', 'imp_hour/1', 'imp_hour/2', 'imp_hour/3', 'imp_hour/4', 'imp_hour/5', 'imp_hour/6', 'imp_hour/7']\n",
            "y feature's name: click_label\n",
            "\n",
            "# of train_data's rows: 1144850 \n",
            "# of test_data's rows: 490650\n",
            "train:test ratio = 0.7:0.3\n",
            "Current Batch Size: 256\n",
            "train_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "test_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "Start Training: Batch Size: 256, Embedding Size: 10\n",
            "Epoch 1: 누적 Loss: 0.5004, Acc: 0.7263, AUC: 0.8169\n",
            "Epoch 2: 누적 Loss: 0.4723, Acc: 0.7464, AUC: 0.8399\n",
            "Epoch 3: 누적 Loss: 0.4586, Acc: 0.7558, AUC: 0.8496\n",
            "Epoch 4: 누적 Loss: 0.4485, Acc: 0.7639, AUC: 0.8575\n",
            "Epoch 5: 누적 Loss: 0.4402, Acc: 0.7704, AUC: 0.8638\n",
            "Epoch 6: 누적 Loss: 0.4505, Acc: 0.7673, AUC: 0.8588\n",
            "Epoch 7: 누적 Loss: 0.4346, Acc: 0.7763, AUC: 0.8689\n",
            "Epoch 8: 누적 Loss: 0.4286, Acc: 0.7811, AUC: 0.8731\n",
            "Epoch 9: 누적 Loss: 0.4238, Acc: 0.7843, AUC: 0.8760\n",
            "Epoch 10: 누적 Loss: 0.4204, Acc: 0.7863, AUC: 0.8782\n",
            "End of Training\n",
            "테스트 ACC: 0.7889, AUC: 0.8802\n",
            "Batch Size: 256, Embedding Size: 10\n",
            "걸린 시간: 31분 40초\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  training(10) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rTf4WNUkI2H",
        "outputId": "8f049d83-2658-44a2-bdab-5b6472a3951b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Data info ----\n",
            "Data Frame shape: (842463, 101)\n",
            "# of Feature: 101\n",
            "# of Field: 27\n",
            "X features' name (10): ['user_gender/F', 'user_gender/M', 'imp_hour/0', 'imp_hour/1', 'imp_hour/2', 'imp_hour/3', 'imp_hour/4', 'imp_hour/5', 'imp_hour/6', 'imp_hour/7']\n",
            "y feature's name: click_label\n",
            "\n",
            "# of train_data's rows: 1144850 \n",
            "# of test_data's rows: 490650\n",
            "train:test ratio = 0.7:0.3\n",
            "Current Batch Size: 512\n",
            "train_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "test_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "Start Training: Batch Size: 512, Embedding Size: 10\n",
            "Epoch 1: 누적 Loss: 0.5006, Acc: 0.7323, AUC: 0.8195\n",
            "Epoch 2: 누적 Loss: 0.4643, Acc: 0.7548, AUC: 0.8463\n",
            "Epoch 3: 누적 Loss: 0.4522, Acc: 0.7652, AUC: 0.8562\n",
            "Epoch 4: 누적 Loss: 0.4420, Acc: 0.7732, AUC: 0.8640\n",
            "Epoch 5: 누적 Loss: 0.4334, Acc: 0.7793, AUC: 0.8700\n",
            "Epoch 6: 누적 Loss: 0.4255, Acc: 0.7853, AUC: 0.8759\n",
            "Epoch 7: 누적 Loss: 0.4198, Acc: 0.7894, AUC: 0.8798\n",
            "Epoch 8: 누적 Loss: 0.4151, Acc: 0.7924, AUC: 0.8828\n",
            "Epoch 9: 누적 Loss: 0.4123, Acc: 0.7944, AUC: 0.8848\n",
            "Epoch 10: 누적 Loss: 0.4092, Acc: 0.7967, AUC: 0.8867\n",
            "End of Training\n",
            "테스트 ACC: 0.7961, AUC: 0.8859\n",
            "Batch Size: 512, Embedding Size: 10\n",
            "걸린 시간: 20분 28초\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  training(10) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbjC-QeKl_3q",
        "outputId": "67a95294-ea3a-4fd8-d137-1f397bb0a633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Data info ----\n",
            "Data Frame shape: (842463, 101)\n",
            "# of Feature: 101\n",
            "# of Field: 27\n",
            "X features' name (10): ['user_gender/F', 'user_gender/M', 'imp_hour/0', 'imp_hour/1', 'imp_hour/2', 'imp_hour/3', 'imp_hour/4', 'imp_hour/5', 'imp_hour/6', 'imp_hour/7']\n",
            "y feature's name: click_label\n",
            "\n",
            "# of train_data's rows: 1144850 \n",
            "# of test_data's rows: 490650\n",
            "train:test ratio = 0.7:0.3\n",
            "Current Batch Size: 512\n",
            "train_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "test_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "Start Training: Batch Size: 512, Embedding Size: 50\n",
            "Epoch 1: 누적 Loss: 0.5072, Acc: 0.7275, AUC: 0.8143\n",
            "Epoch 2: 누적 Loss: 0.4692, Acc: 0.7510, AUC: 0.8424\n",
            "Epoch 3: 누적 Loss: 0.4544, Acc: 0.7625, AUC: 0.8548\n",
            "Epoch 4: 누적 Loss: 0.4443, Acc: 0.7701, AUC: 0.8617\n",
            "Epoch 5: 누적 Loss: 0.4379, Acc: 0.7750, AUC: 0.8665\n",
            "Epoch 6: 누적 Loss: 0.4318, Acc: 0.7793, AUC: 0.8705\n",
            "Epoch 7: 누적 Loss: 0.4277, Acc: 0.7831, AUC: 0.8737\n",
            "Epoch 8: 누적 Loss: 0.4297, Acc: 0.7820, AUC: 0.8730\n",
            "Epoch 9: 누적 Loss: 0.4196, Acc: 0.7882, AUC: 0.8788\n",
            "Epoch 10: 누적 Loss: 0.4155, Acc: 0.7907, AUC: 0.8819\n",
            "End of Training\n",
            "테스트 ACC: 0.7894, AUC: 0.8829\n",
            "Batch Size: 512, Embedding Size: 50\n",
            "걸린 시간: 33분 60초\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  training(10) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilh7MZyMxaBv",
        "outputId": "7a06d9a1-5fa6-4d2d-a1ae-40d5b7742573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Data info ----\n",
            "Data Frame shape: (842463, 101)\n",
            "# of Feature: 101\n",
            "# of Field: 27\n",
            "X features' name (10): ['user_gender/F', 'user_gender/M', 'imp_hour/0', 'imp_hour/1', 'imp_hour/2', 'imp_hour/3', 'imp_hour/4', 'imp_hour/5', 'imp_hour/6', 'imp_hour/7']\n",
            "y feature's name: click_label\n",
            "\n",
            "# of train_data's rows: 1144850 \n",
            "# of test_data's rows: 490650\n",
            "train:test ratio = 0.7:0.3\n",
            "Current Batch Size: 1024\n",
            "train_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "test_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "Start Training: Batch Size: 1024, Embedding Size: 10\n",
            "Epoch 1: 누적 Loss: 0.4957, Acc: 0.7292, AUC: 0.8174\n",
            "Epoch 2: 누적 Loss: 0.4587, Acc: 0.7538, AUC: 0.8462\n",
            "Epoch 3: 누적 Loss: 0.4458, Acc: 0.7648, AUC: 0.8567\n",
            "Epoch 4: 누적 Loss: 0.4375, Acc: 0.7715, AUC: 0.8629\n",
            "Epoch 5: 누적 Loss: 0.4319, Acc: 0.7756, AUC: 0.8673\n",
            "Epoch 6: 누적 Loss: 0.4273, Acc: 0.7793, AUC: 0.8706\n",
            "Epoch 7: 누적 Loss: 0.4238, Acc: 0.7817, AUC: 0.8730\n",
            "Epoch 8: 누적 Loss: 0.4206, Acc: 0.7844, AUC: 0.8753\n",
            "Epoch 9: 누적 Loss: 0.4180, Acc: 0.7863, AUC: 0.8774\n",
            "Epoch 10: 누적 Loss: 0.4165, Acc: 0.7874, AUC: 0.8784\n",
            "End of Training\n",
            "테스트 ACC: 0.7866, AUC: 0.8778\n",
            "Batch Size: 1024, Embedding Size: 10\n",
            "걸린 시간: 12분 37초\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 활성화 함수 = tanh일 때\n",
        "class DeepFM(tf.keras.Model):\n",
        "    def __init__(self, num_feature, grouped_field, embedding_size, feature_index):\n",
        "        super(DeepFM, self).__init__()\n",
        "        self.embedding_size = embedding_size    # k: 임베딩 벡터의 차원(크기)\n",
        "        self.num_feature = num_feature          # f: 원래 feature 개수\n",
        "        self.grouped_field = grouped_field      # m: grouped field 개수\n",
        "        self.feature_index = feature_index      # 인코딩된 X의 칼럼들이 본래 어디 소속이었는지 (칼럼 인덱스)\n",
        "\n",
        "        self.fm_layer = FM_layer(num_feature, grouped_field, embedding_size, feature_index)\n",
        "        \n",
        "        self.hidden_layer1 = tf.keras.layers.Dense(units=64, activation='tanh') #tf.keras.layers.Dense(레이어 사이즈, 활성화 함수): 인공신경망 구조를 구현시켜주는 함수\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate=0.5)\n",
        "        self.hidden_layer2 = tf.keras.layers.Dense(units=32, activation='tanh')\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate=0.5)\n",
        "        self.hidden_layer3 = tf.keras.layers.Dense(units=16, activation='tanh')\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate=0.1)\n",
        "        self.hidden_layer4 = tf.keras.layers.Dense(units=2, activation='tanh')\n",
        "\n",
        "        self.output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"DeepFM Model: # Field: {}, # Feature: {}, Embedding: {}\".format(self.grouped_field, self.num_feature, self.embedding_size)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        # 1) FM Component: (num_batch, 2)\n",
        "        y_fm, vector_inputs = self.fm_layer(inputs)\n",
        "        # print(f'Sigmoid Function shape: {y_fm.shape}') #Sigmoid Function shape: (256, 2)\n",
        "        # print(f'input vector shape: {vector_inputs.shape}') #input vector shape: (256, 101, 10)\n",
        "\n",
        "        # retrieve Dense Vectors: (num_batch, num_feature*embedding_size)\n",
        "        vector_inputs = tf.reshape(vector_inputs, [-1, self.num_feature*self.embedding_size])\n",
        "\n",
        "        # 2) Deep Component\n",
        "        y_deep = self.hidden_layer1(vector_inputs)\n",
        "        y_deep = self.dropout1(y_deep)\n",
        "        y_deep = self.hidden_layer2(y_deep)\n",
        "        y_deep = self.dropout2(y_deep)\n",
        "        y_deep = self.hidden_layer3(y_deep)\n",
        "        y_deep = self.dropout3(y_deep)\n",
        "        y_deep = self.hidden_layer4(y_deep)\n",
        "\n",
        "        # Concatenation\n",
        "        y_pred = tf.concat([y_fm, y_deep], 1)\n",
        "        y_pred = self.output_layer(y_pred)\n",
        "        y_pred = tf.reshape(y_pred, [-1, ])\n",
        "        \n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "WKE3DyBKhbtC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 활성화 함수 = 탄젠트일 때 결과\n",
        "if __name__ == '__main__':\n",
        "  training(10) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pm0s3yziByg",
        "outputId": "46f21489-e917-41f9-a539-9ee9c64b545e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Data info ----\n",
            "Data Frame shape: (842463, 101)\n",
            "# of Feature: 101\n",
            "# of Field: 27\n",
            "X features' name (10): ['user_gender/F', 'user_gender/M', 'imp_hour/0', 'imp_hour/1', 'imp_hour/2', 'imp_hour/3', 'imp_hour/4', 'imp_hour/5', 'imp_hour/6', 'imp_hour/7']\n",
            "y feature's name: click_label\n",
            "\n",
            "# of train_data's rows: 1144850 \n",
            "# of test_data's rows: 490650\n",
            "train:test ratio = 0.7:0.3\n",
            "Current Batch Size: 512\n",
            "train_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "test_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "Start Training: Batch Size: 512, Embedding Size: 10\n",
            "Epoch 1: 누적 Loss: 0.4975, Acc: 0.7251, AUC: 0.8135\n",
            "Epoch 2: 누적 Loss: 0.4666, Acc: 0.7463, AUC: 0.8383\n",
            "Epoch 3: 누적 Loss: 0.4576, Acc: 0.7540, AUC: 0.8459\n",
            "Epoch 4: 누적 Loss: 0.4523, Acc: 0.7591, AUC: 0.8505\n",
            "Epoch 5: 누적 Loss: 0.4490, Acc: 0.7622, AUC: 0.8535\n",
            "Epoch 6: 누적 Loss: 0.4473, Acc: 0.7641, AUC: 0.8551\n",
            "Epoch 7: 누적 Loss: 0.4452, Acc: 0.7660, AUC: 0.8569\n",
            "Epoch 8: 누적 Loss: 0.4444, Acc: 0.7665, AUC: 0.8576\n",
            "Epoch 9: 누적 Loss: 0.4446, Acc: 0.7664, AUC: 0.8576\n",
            "Epoch 10: 누적 Loss: 0.4441, Acc: 0.7676, AUC: 0.8582\n",
            "End of Training\n",
            "테스트 ACC: 0.7639, AUC: 0.8565\n",
            "Batch Size: 512, Embedding Size: 10\n",
            "걸린 시간: 12분 32초\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  training(10) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc4jYWzolw9N",
        "outputId": "cbf44abb-d430-4ebd-d9d6-9374f2bf7918"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Data info ----\n",
            "Data Frame shape: (842463, 101)\n",
            "# of Feature: 101\n",
            "# of Field: 27\n",
            "X features' name (10): ['user_gender/F', 'user_gender/M', 'imp_hour/0', 'imp_hour/1', 'imp_hour/2', 'imp_hour/3', 'imp_hour/4', 'imp_hour/5', 'imp_hour/6', 'imp_hour/7']\n",
            "y feature's name: click_label\n",
            "\n",
            "# of train_data's rows: 1144850 \n",
            "# of test_data's rows: 490650\n",
            "train:test ratio = 0.7:0.3\n",
            "Current Batch Size: 512\n",
            "train_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "test_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "Start Training: Batch Size: 512, Embedding Size: 50\n",
            "Epoch 1: 누적 Loss: 0.5063, Acc: 0.7210, AUC: 0.8078\n",
            "Epoch 2: 누적 Loss: 0.4831, Acc: 0.7409, AUC: 0.8302\n",
            "Epoch 3: 누적 Loss: 0.4685, Acc: 0.7484, AUC: 0.8388\n",
            "Epoch 4: 누적 Loss: 0.4668, Acc: 0.7502, AUC: 0.8401\n",
            "Epoch 5: 누적 Loss: 0.4679, Acc: 0.7490, AUC: 0.8393\n",
            "Epoch 6: 누적 Loss: 0.4685, Acc: 0.7488, AUC: 0.8392\n",
            "Epoch 7: 누적 Loss: 0.4700, Acc: 0.7482, AUC: 0.8382\n",
            "Epoch 8: 누적 Loss: 0.4709, Acc: 0.7473, AUC: 0.8377\n",
            "Epoch 9: 누적 Loss: 0.4717, Acc: 0.7464, AUC: 0.8371\n",
            "Epoch 10: 누적 Loss: 0.4716, Acc: 0.7468, AUC: 0.8368\n",
            "End of Training\n",
            "테스트 ACC: 0.7477, AUC: 0.8378\n",
            "Batch Size: 512, Embedding Size: 50\n",
            "걸린 시간: 25분 22초\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  training(10) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUW21p0wtXUU",
        "outputId": "f6607014-6792-4dc3-dc38-28709e706cde"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Data info ----\n",
            "Data Frame shape: (842463, 101)\n",
            "# of Feature: 101\n",
            "# of Field: 27\n",
            "X features' name (10): ['user_gender/F', 'user_gender/M', 'imp_hour/0', 'imp_hour/1', 'imp_hour/2', 'imp_hour/3', 'imp_hour/4', 'imp_hour/5', 'imp_hour/6', 'imp_hour/7']\n",
            "y feature's name: click_label\n",
            "\n",
            "# of train_data's rows: 1144850 \n",
            "# of test_data's rows: 490650\n",
            "train:test ratio = 0.7:0.3\n",
            "Current Batch Size: 1024\n",
            "train_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "test_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "Start Training: Batch Size: 1024, Embedding Size: 10\n",
            "Epoch 1: 누적 Loss: 0.5091, Acc: 0.7173, AUC: 0.8038\n",
            "Epoch 2: 누적 Loss: 0.4716, Acc: 0.7444, AUC: 0.8357\n",
            "Epoch 3: 누적 Loss: 0.4560, Acc: 0.7570, AUC: 0.8485\n",
            "Epoch 4: 누적 Loss: 0.4472, Acc: 0.7641, AUC: 0.8553\n",
            "Epoch 5: 누적 Loss: 0.4414, Acc: 0.7688, AUC: 0.8597\n",
            "Epoch 6: 누적 Loss: 0.4374, Acc: 0.7722, AUC: 0.8629\n",
            "Epoch 7: 누적 Loss: 0.4352, Acc: 0.7741, AUC: 0.8644\n",
            "Epoch 8: 누적 Loss: 0.4324, Acc: 0.7765, AUC: 0.8665\n",
            "Epoch 9: 누적 Loss: 0.4301, Acc: 0.7786, AUC: 0.8681\n",
            "Epoch 10: 누적 Loss: 0.4298, Acc: 0.7782, AUC: 0.8684\n",
            "End of Training\n",
            "테스트 ACC: 0.7709, AUC: 0.8679\n",
            "Batch Size: 1024, Embedding Size: 10\n",
            "걸린 시간: 10분 41초\n",
            "\n"
          ]
        }
      ]
    }
  ]
}