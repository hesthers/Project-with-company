{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FM model - ë²ˆê°œì¥í„°.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "157yvDeQI43e_PvEiUZSo9dcjPXlk9D4D",
      "authorship_tag": "ABX9TyOfmPyf//VSbouW0peBcyna"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° ë¡œë“œ"
      ],
      "metadata": {
        "id": "LTftVsyFyUdZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo3cEsKZEPZj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from itertools import repeat\n",
        "from time import perf_counter\n",
        "from tensorflow.keras.metrics import BinaryAccuracy, AUC\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
        "pd.set_option('display.max_columns', None)\n",
        "tf.keras.backend.set_floatx('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì „ì²˜ë¦¬ ì½”ë“œ"
      ],
      "metadata": {
        "id": "EJ6nTKzXyOZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë…¼ë¬¸ ë²ˆì—­ ë°œì·Œ\n",
        "- Ï‡\n",
        "> ìœ ì €ì™€ ì•„ì´í…œì„ ë¬¶ëŠ” m-fields(ì¢…ë¥˜)ë¡œ êµ¬ì„±\n",
        "> ë²”ì£¼í˜• field(e.g. ì„±ë³„, ìœ„ì¹˜)ë¥¼ í¬í•¨í•  ìˆ˜ë„ ìˆê³  ì—°ì†í˜• field(e.g. ë‚˜ì´)ë¥¼ í¬í•¨\n",
        "\n",
        "- y\n",
        "> ìœ ì €ê°€ í´ë¦­ì„ í–ˆëŠ”ì§€ ì•ˆ í–ˆëŠ”ì§€ì˜ ë°ì´í„°ê°€ 0,1ë¡œ ë¼ë²¨ë§  \n",
        "\n",
        "- ê°ê°ì˜ ë²”ì£¼í˜• fieldëŠ” one-hot vectorë¡œ í‘œí˜„\n",
        "- ê°ê°ì˜ ì—°ì†í˜• fieldëŠ” ê°’ ìì²´ë¡œ í‘œí˜„ë˜ê±°ë‚˜ ì´ì‚°í™”(discretization)í•œ ë’¤ì— one-hot encdoingì„ í•˜ì—¬ í‘œí˜„.\n",
        "\n",
        "- ê·¸ëŸ° ë‹¤ìŒ ê°ê°ì˜ ë°ì´í„°ëŠ” ( ğ‘¥ ,y) shapeìœ¼ë¡œ ë°”ë€ŒëŠ”ë°  ğ‘¥  = [ $ğ‘¥_{ğ‘“ğ‘–ğ‘’ğ‘™ğ‘‘1} , ğ‘¥_{ğ‘“ğ‘–ğ‘’ğ‘™ğ‘‘2} , ... ,  ğ‘¥_{ğ‘“ğ‘–ğ‘’ğ‘™ğ‘‘ğ‘—} ,... ,  ğ‘¥_{ğ‘“ğ‘–ğ‘’ğ‘™ğ‘‘ğ‘š}$ ] ì´ê³  \n",
        "dì°¨ì›ì˜ ë²¡í„°  \n",
        "- $ğ‘¥_{ğ‘“ğ‘–ğ‘’ğ‘™ğ‘‘ğ‘—}$  ëŠ”  ğœ’  ì˜ jë²ˆì§¸ fieldë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. \n",
        "- ë³´í†µ xëŠ” ë§¤ìš° ì°¨ì›ì´ ë†’ê³  ê·¹ë„ë¡œ sparseí•˜ë‹¤."
      ],
      "metadata": {
        "id": "YGxRcGMpyDEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 256\n",
        "EMBEDDING_SIZE = 10\n",
        "\n",
        "def data_split():\n",
        "    cols = {'obj': [],\n",
        "        'cat': [],\n",
        "       'cont': []\n",
        "        }\n",
        "\n",
        "    file_name = 'final_data_v2.csv'\n",
        "    file_path = os.getcwd()+'/drive/MyDrive/Colab Notebooks/' #[:-16]ëŠ” ë³¸ì¸ ê²½ë¡œì— ë§ê²Œ ìˆì–´ë„ ë˜ê³  ì—†ì–´ë„ ë¨.\n",
        "    df = pd.read_csv(file_path+file_name, encoding='utf-8')\n",
        "    df.rename(columns={'category_id_1':'category1'}, inplace=True)\n",
        "\n",
        "    # ë°ì´í„° ìœ í˜•ë³„ ë¶„ë¥˜í•˜ê¸°\n",
        "    for dt_idx, dt_val in zip(df.dtypes.index, df.dtypes.values):\n",
        "        # if 'category' in dt_idx:\n",
        "        #     df[['category1']] = LabelEncoder().fit_transform(df[['category1']])\n",
        "        #     cols['cat'].append('category1')\n",
        "\n",
        "        if dt_val == 'object':\n",
        "            if ('id' in dt_idx) | ('time' in dt_idx) | ('name' in dt_idx) | ('keyword' in dt_idx) |('url' in dt_idx):\n",
        "                df.drop(columns = dt_idx, axis=1, inplace=True)\n",
        "            else:\n",
        "                cols['obj'].append(dt_idx)\n",
        "\n",
        "        else:\n",
        "            if ('id' in dt_idx) | ('time' in dt_idx):\n",
        "                df.drop(columns = dt_idx, axis=1, inplace=True)\n",
        "            else:\n",
        "                if len(df[dt_idx].value_counts()) <= 30: #ì—°ì†í˜• ë°ì´í„° ì¤‘ 30ê°œ ë‚´ì˜ ë²”ì£¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ” ë°ì´í„° = categoryë¡œ êµ¬ë¶„.\n",
        "                    cols['cat'].append(dt_idx)\n",
        "                else:\n",
        "                    if ('hour' in dt_idx) | ('group' in dt_idx):\n",
        "                        pass\n",
        "                    else:\n",
        "                        cols['cont'].append(dt_idx) \n",
        "\n",
        "    return cols"
      ],
      "metadata": {
        "id": "j_u_CUVOFSCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reorganization(df):\n",
        "  data = pd.DataFrame()\n",
        "  cols = data_split()\n",
        "  for k, v in cols.items():\n",
        "    if k == 'obj':\n",
        "      data = pd.concat([data, df[v]], axis=1)\n",
        "    elif k == 'cont':\n",
        "      data = pd.concat([data, df[v]], axis=1)\n",
        "    else:\n",
        "      data = pd.concat([data, df[v]], axis=1)\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "1CxNB2uSEt0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing():\n",
        "    file_name = 'final_data_v2.csv'\n",
        "    file_path = os.getcwd()+'/drive/MyDrive/Colab Notebooks/' \n",
        "    df = pd.read_csv(file_path+file_name, encoding='utf-8')\n",
        "    df.rename(columns={'category_id_1':'category1'}, inplace=True)\n",
        "    # ë°ì´í„° ìœ í˜•ë³„ ë¶„ë¥˜í•˜ê¸°\n",
        "    data = reorganization(df)\n",
        "    # cols = data_split()\n",
        "    modified_df = pd.DataFrame()\n",
        "    vec_dict = {idx: [] for idx in range(len(data.columns))}\n",
        "    feature_index = []\n",
        "\n",
        "    for i, c in enumerate(data.columns):\n",
        "        if c in cols['obj']:\n",
        "            obj_data = pd.get_dummies(data[c], prefix=c, prefix_sep = \"/\")\n",
        "            modified_df = pd.concat([modified_df, obj_data], axis=1)\n",
        "            vec_dict[i] = list(obj_data.columns)\n",
        "            feature_index.extend(repeat(i, obj_data.shape[1]))\n",
        "\n",
        "        elif c in cols['cat']:  # click_label ì»¬ëŸ¼ = y ë³€ìˆ˜ë¡œ ì‚¬ìš©\n",
        "            if 'click' in c:\n",
        "                pass\n",
        "            else:\n",
        "                cat_data = pd.get_dummies(data[c], prefix=c, prefix_sep = \"/\")\n",
        "                vec_dict[i] = list(cat_data.columns)\n",
        "                feature_index.extend(repeat(i, cat_data.shape[1]))\n",
        "                modified_df = pd.concat([modified_df, cat_data], axis=1)\n",
        "        else:\n",
        "            scaled_num_data = MinMaxScaler().fit_transform(df[[c]])\n",
        "            scaled_num_data = pd.DataFrame(scaled_num_data, columns = [c])\n",
        "            modified_df = pd.concat([modified_df,scaled_num_data], axis=1)\n",
        "            vec_dict[i] = list(scaled_num_data.columns)\n",
        "            feature_index.extend(repeat(i, scaled_num_data.shape[1]))\n",
        "\n",
        "    print('---- Data info ----')\n",
        "    print(cols)\n",
        "    print('Data Frame shape: {}'.format(modified_df.shape))\n",
        "    print('# of Feature: {}'.format(len(feature_index)))\n",
        "    print(f'# of Field: {len(vec_dict)}')\n",
        "    print(f'Modified DF columns: {modified_df.columns}')\n",
        "    # print(vec_dict)\n",
        "    return vec_dict, feature_index, modified_df"
      ],
      "metadata": {
        "id": "sr2X18tpasbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vec_dict, feature_index, modified_df = preprocessing()"
      ],
      "metadata": {
        "id": "oWuMedsPAlfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modified_df.head(3)"
      ],
      "metadata": {
        "id": "GSUwCGwuQvTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ëª¨ë¸ë§ì„ ìœ„í•œ ë°ì´í„° ë¶„ë¦¬í•˜ê¸°"
      ],
      "metadata": {
        "id": "K84d3D2wzP9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° 7:3ìœ¼ë¡œ ë‚˜ëˆ„ê¸° (í˜¹ì€ 8:2)\n",
        "def split_data():\n",
        "    file_name = 'final_data_v2.csv'\n",
        "    file_path = os.getcwd()+'/drive/MyDrive/Colab Notebooks/' \n",
        "    df = pd.read_csv(file_path+file_name, encoding='utf-8')\n",
        "    df.rename(columns={'category_id_1':'category1'}, inplace=True)\n",
        "\n",
        "    vec_dict, feature_index, modified_df = preprocessing()\n",
        "\n",
        "    X = modified_df#.astype('float')\n",
        "    y = df['click_label']\n",
        "\n",
        "    print(f\"X features' name (10): {X.columns.to_list()[:10]}\")\n",
        "    print(f\"y feature's name: {y.name}\")\n",
        "    print()\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 2022, stratify = y) #y ë¹„ìœ¨ì— ë”°ë¥¸ ì¸µí™”ì¶”ì¶œ ë° ë°ì´í„°ë¥¼ 7:3ì˜ ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ê¸°\n",
        "    \n",
        "    print(f\"# of train_data's rows: {X_train.shape[0]} \\n# of test_data's rows: {X_test.shape[0]}\")\n",
        "    print(f'train:test ratio = {round(X_train.shape[0]/(X_train.shape[0]+ X_test.shape[0]),2)}:{round(X_test.shape[0]/(X_train.shape[0]+ X_test.shape[0]), 2)}')\n",
        "    # return X_train, X_test, y_train, y_test\n",
        "\n",
        "    # tf.data.Dataset.from_tensor_slices í•¨ìˆ˜: tf.data.Dataset ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë¡œ ì…ë ¥ëœ í…ì„œë¡œë¶€í„° slicesë¥¼ ìƒì„±.\n",
        "    # shuffle í•¨ìˆ˜ëŠ” ê³ ì •ëœ ë²„í¼ í¬ê¸°ë¡œ ë°ì´í„°ë¥¼ ì„ëŠ”ë°, ë°ì´í„°ê°€ ì™„ì „íˆ ëœë¤ì ìœ¼ë¡œ ë’¤ì„ê¸° ìœ„í•´ì„œëŠ” ì…ë ¥ëœ ë°ì´í„° í¬ê¸°ë³´ë‹¤ í° ìˆ˜ë¥¼ ì…ë ¥.\n",
        "    # tf.cast í•¨ìˆ˜: ë’¤ì— ë‚˜ì˜¨ í˜•ìœ¼ë¡œ ê°’ì„ ë³€í™˜\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices( \n",
        "              (tf.cast(X_train.values, tf.float32), tf.cast(y_train, tf.float32))\n",
        "            ).shuffle(600000).batch(BATCH_SIZE) \n",
        "    \n",
        "    test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "              (tf.cast(X_test.values, tf.float32), tf.cast(y_test, tf.float32))\n",
        "            ).shuffle(300000).batch(BATCH_SIZE)\n",
        "    \n",
        "    print(f'Current Batch Size: {BATCH_SIZE}')\n",
        "    print(f'train_ds: {train_ds}')\n",
        "    print(f'test_ds: {test_ds}')\n",
        "    return train_ds, test_ds, vec_dict, feature_index"
      ],
      "metadata": {
        "id": "tS5mQa-Fz1ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split_data()"
      ],
      "metadata": {
        "id": "YuS4CepGQ5Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeepFM model ì½”ë“œ"
      ],
      "metadata": {
        "id": "dYKL_hKn6Fk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeepFMì€ ê°™ì€ inputì„ ê³µìœ í•˜ëŠ” FM componentì™€ deep componentë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. \n",
        "\n",
        "$<FM component>$\n",
        "\n",
        "í”¼ì³ iì— ëŒ€í•´ì„œ, **ìŠ¤ì¹¼ë¼ ê°’ $w_i$ëŠ” order-1 ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜**ë¡œ ì‚¬ìš©ë˜ê³  **ì ì¬ ë²¡í„°(latent vector) $V_i$ëŠ” ë‹¤ë¥¸ í”¼ì³ì™€ ìƒí˜¸ì‘ìš© ì •ë„ë¥¼ ì¸¡ì •**í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤. $V_i$ëŠ” order-2 ìƒí˜¸ì‘ìš©ì„ ëª¨ë¸ë§í•˜ê¸° ìœ„í•´ì„œ FM componentë¡œ ë“¤ì–´ê°€ëŠ” ë°˜ë©´ ë†’ì€ ì°¨ì›ì˜ í”¼ì³ ìƒí˜¸ì‘ìš©ì„ ëª¨ë¸ë§í•˜ê¸° ìœ„í•´ì„œ Deep Componentë¡œ ë“¤ì–´ê°„ë‹¤. $w_i$ ì™€ $V_i$ ê·¸ë¦¬ê³  network ë³€ìˆ˜$(W^{l}, b^{l})$ì„ í¬í•¨í•œ ëª¨ë“  ë³€ìˆ˜ëŠ” ì•„ë˜ ìˆ˜ì‹ê³¼ ê°™ì´ ê²°í•©ëœ ì˜ˆì¸¡ ëª¨ë¸ì—ì„œ ê°™ì´ í•™ìŠµëœë‹¤.\n",
        "\n",
        "$<FM model - based>$\n",
        "$\\hat{y}(x) := w_0 + \\sum^n_{i=1}w_i x_i + \\sum^n_{i=1}\\sum^n_{j=i+1} \\langle{v_i,v_j}\\rangle x_ix_j \\tag{1}$\n",
        "$\\langle{v_i,v_j}\\rangle := \\sum^k_{f=1}v_{i,f} \\cdot v_{j,f}\\tag{2}$\n",
        "\n",
        "$<Deep FM model>$\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "Parameter wì˜ ê¸¸ì´ëŠ” num_feature(27)ì´ë©°, Parameter Vì˜ shapeì€ num_field(102), embedding_size(ì‚¬ìš©ì ì§€ì •)ì´ë‹¤. \n",
        "\n",
        "call í•¨ìˆ˜ì—ì„œ ë³´ë©´ ì•Œ ìˆ˜ ìˆë“¯ì´, ì´ Ví–‰ë ¬ì€ One-Hot ì¸ì½”ë”©ëœ ë°ì´í„°ì— ê³±í•´ì§€ëŠ” êµ¬ì¡°ì´ê¸° ë•Œë¬¸ì— tf.nn.embedding_lookupì´ë¼ëŠ” í•¨ìˆ˜ë¥¼ í†µí•´ í–‰ì´ ë³µì œ/ ì¦‰, ì•ì„œ ìƒì„±í•œ field_indexì˜ ì •ë³´ë¥¼ ì°¸ì¡°í•˜ì—¬, ê°™ì€ fieldì—ì„œ ë‚˜ì˜¨ featureì¼ ê²½ìš°, ê°™ì€ Embedding Row(Vì˜ Row)ë¥¼ ê³µìœ . \n",
        "\n",
        "num_feature=len(field_index) = num_field=len(field_dict) # columnsì˜ ìˆ˜"
      ],
      "metadata": {
        "id": "tL9OLxta6UbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FM_layer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_feature, grouped_field, embedding_size, feature_index):\n",
        "        super(FM_layer, self).__init__()\n",
        "        self.embedding_size = embedding_size    # k: ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›(í¬ê¸°)\n",
        "        self.num_feature = num_feature          # f: ì›ë˜ feature ê°œìˆ˜\n",
        "        self.grouped_field = grouped_field      # m: grouped field ê°œìˆ˜\n",
        "        self.feature_index = feature_index      # ì¸ì½”ë”©ëœ Xì˜ ì¹¼ëŸ¼ë“¤ì´ ë³¸ë˜ ì–´ë”” ì†Œì†ì´ì—ˆëŠ”ì§€\n",
        "\n",
        "        # Parameters of FM Layer\n",
        "        # w: capture 1st order interactions (linear - num_feature ë§Œí¼ì˜ í¬ê¸°ë¥¼ ê°€ì§„ ë²¡í„°)\n",
        "        # V: capture 2nd order interactions\n",
        "        # tf.Variable: ëª¨ë¸ë§ì—ì„œ weightë‚˜ biasì™€ ê°™ì€ ë³€ìˆ˜ ê°’ì„ ì´ˆê¸°í™”í•˜ëŠ” í›ˆë ¨ê°€ëŠ¥í•œ ë³€ìˆ˜\n",
        "        self.w = tf.Variable(tf.random.normal(shape=[num_feature], mean=0.0, stddev=1.0)\n",
        "                           , name='w'\n",
        "                            )\n",
        "        self.V = tf.Variable(tf.random.normal(shape=(grouped_field, embedding_size), \n",
        "                                              mean=0.0, stddev=0.01)\n",
        "                           , name='V'\n",
        "                            )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # try:\n",
        "          # print(f\"input vector's shape: {inputs.shape}\") #input vector's shape: (256, 101)\n",
        "        x_batch = tf.reshape(tf.expand_dims(inputs, axis=-1), [-1, self.num_feature, 1]) #-1: ê°€ë¡œ vectorë¡œ ìƒì„±, self.feature_index ë§Œí¼ì˜ í–‰, 1ì—´\n",
        "        # print(f\"X's Batch Shape: {x_batch.shape}\") #X's Batch Shape: (256, 101, 1)                 \n",
        "        # except ValueError as exception:\n",
        "        #   print(exception)\n",
        "        \n",
        "        # Parameter Vë¥¼ feature_indexì— ë§ê²Œ ë³µì‚¬í•˜ì—¬ num_featureì— ë§ê²Œ ëŠ˜ë¦¼ (field ìˆ˜ë§Œí¼ embedding)\n",
        "        embeds = tf.nn.embedding_lookup(params=self.V, ids=self.feature_index)\n",
        "        # print(f\"Embedding Layer Shape: {embeds.shape}\") #Embedding Layer Shape: (101, 10)\n",
        "        \n",
        "        # Deep Componentì—ì„œ ì“¸ Input\n",
        "        # (batch_size, num_feature, embedding_size)\n",
        "        # order-2 layer (inner product of respective feature latent vectors)\n",
        "        vector_inputs = tf.math.multiply(x_batch, embeds) \n",
        "        # print(f'Input Layer Shape: {vector_inputs.shape}') #Input Layer Shape: (256, 101, 10)\n",
        "\n",
        "        # (batch_size, ) -> order-1 layer (linear interactions among features)\n",
        "        linear_terms = tf.reduce_sum(tf.math.multiply(self.w, inputs), axis=1, keepdims=False)\n",
        "\n",
        "        # (batch_size, ) -> order-2 features (inner product units)\n",
        "        # tf.math.pow(tf.matmul(inputs, self.V), 2) - tf.matmul(tf.math.pow(inputs, 2), tf.math.pow(self.V, 2)\n",
        "        # tf.matmul: í–‰ë ¬ì˜ ê³±ì…ˆ\n",
        "        interactions = 0.5 * tf.subtract(\n",
        "            tf.square(tf.reduce_sum(vector_inputs, [1, 2])),\n",
        "            tf.reduce_sum(tf.square(vector_inputs), [1, 2])\n",
        "        )\n",
        "        \n",
        "        # sigmoid function for CTR prediction\n",
        "        linear_terms = tf.reshape(linear_terms, [-1, 1]) #ë²¡í„° -> tensorí™”ëœ ë²¡í„°ë¡œ ë³€í™˜\n",
        "        interactions = tf.reshape(interactions, [-1, 1])\n",
        "        y_fm = tf.concat([linear_terms, interactions], 1)\n",
        "\n",
        "        return y_fm, vector_inputs"
      ],
      "metadata": {
        "id": "L19WQYWNES8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding Layer êµ¬ì¡°ì—ëŠ” ë‘ ê°€ì§€ í¥ë¯¸ë¡œìš´ ì ì´ ìˆë‹¤. \n",
        "\n",
        "`an embedding layer to compress the input vector to a low-dimensional,dense real-value vector before further feeding into the first hidden layer`\n",
        "\n",
        "1) **ë‹¤ë¥¸ fieldì˜ input vectorì˜ ê¸¸ì´ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆì§€ë§Œ embeddingì€ ê°™ì€ í¬ê¸°(k)**ì´ë‹¤. (ì—­ì ì˜ˆì‹œ: gender fieldëŠ” ë³´í†µ lengthê°€ ë‚¨, ì—¬ 2ì¸ ë°˜ë©´ êµ­ì ì´ë‚˜ ë‚˜ì´ fieldì˜ ê¸¸ì´ëŠ” ë” ê¸¸ë‹¤. í•˜ì§€ë§Œ embeddingì‹œì—ëŠ” ë˜‘ê°™ì´ k=5ì°¨ì› ë²¡í„°ë¡œ ì„ë² ë”© ëœë‹¤.) \n",
        "\n",
        "\n",
        "FM componentì™€ Deep componentê°€ ê°™ì€ feature embddingì„ ê³µìœ í•œë‹¤ëŠ” ì ì´ ì£¼ëª©í•  ë§Œ í•œë° ì´ ë•ë¶„ì— ë‘ ê°€ì§€ ì¤‘ìš”í•œ ì¥ì ì„ ê°–ê²Œ ëœë‹¤. \n",
        "\n",
        "1) raw featureë¡œë¶€í„° ë‚®ì€ ì°¨ì›ê³¼ ë†’ì€ ì°¨ì›ì˜ í”¼ì³ ìƒí˜¸ì‘ìš©ì„ ë‘˜ ë‹¤ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. 2) Wide & Deep ëª¨ë¸ê³¼ ë‹¤ë¥´ê²Œ inputì˜ ì§ì ‘ì ì¸ feature engineeringì´ í•„ìš”í•˜ì§€ ì•Šë‹¤.\n",
        "\n",
        "2) **FMì—ì„œì˜ latent feature vector(V)ê°€ ì´ ë„¤íŠ¸ì›Œí¬ì˜ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©ë˜ê³  input field vectorë¥¼ ì••ì¶•í•˜ëŠ” ë° ì‚¬ìš©ë˜ê³  í•™ìŠµ**ëœë‹¤. [Zhang et al., 2016]ì—ì„œëŠ” Vê°€ FMì— ì˜í•´ pre-trainedë˜ê³  ì´ ê°’ì„ ì´ˆê¸°ê°’ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. ì´ë²ˆ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ° ë°©ë²•ì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  FMì„ DNNê³¼ ë³„ê°œë¡œ í•™ìŠµ êµ¬ì¡°ì— í¬í•¨í•œë‹¤. ì´ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ pre-trainingì„ í•  í•„ìš”ê°€ ì—†ì–´ì§€ê³  ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ end-to-endë¡œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ëœë‹¤. Embedding Layerì˜ outputì„ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•˜ë©´:\n",
        "\n",
        "> $a^{(0)}$ = $[e_1, e_2, ..., e_m]$\n",
        "\n",
        "$e_i$ëŠ” ië²ˆì§¸ fieldì˜ ì„ë² ë”©ì„ ë‚˜íƒ€ë‚´ê³  mì€ fieldì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.\n",
        "\n",
        "$a^{(0)}$ì€ DNNì— ë“¤ì–´ê°€ì„œ ë‹¤ìŒê³¼ ê°™ì´ forward processë¥¼ ê±°ì¹œë‹¤:\n",
        "\n",
        "> $a^{(l+1)}$ = $\\sigma (W^{(l)}a^{(l)}+b^{(l)})$\n",
        "\n",
        "- $a^{(l)}$ì€ lë²ˆì§¸ layerì˜ output\n",
        "- $\\sigma$: í™œì„±í™”í•¨ìˆ˜ & $l$: layer depth & $b^{(l)}$: lë²ˆì§¸ layer bias\n",
        "\n",
        "the sigmoid function for CTR prediction: \n",
        "\n",
        "> $y_{DNN}$ = $\\sigma(W^{|H|+1} \\times a^H + b^{|H|+1})$ \n",
        "\n",
        ", where |H| is the number of hidden layers.\n",
        "\n",
        "ì¶œì²˜: https://orill.tistory.com/entry/ë…¼ë¬¸-ë¦¬ë·°-DeepFM-A-Factorization-Machine-based-Neural-Network-for-CTR-Prediction [ì´ì œ ë©°ì¹  í›„ì—”]"
      ],
      "metadata": {
        "id": "bIV27sgY8jfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepFM(tf.keras.Model):\n",
        "    def __init__(self, num_feature, grouped_field, embedding_size, feature_index):\n",
        "        super(DeepFM, self).__init__()\n",
        "        self.embedding_size = embedding_size    # k: ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›(í¬ê¸°)\n",
        "        self.num_feature = num_feature          # f: ì›ë˜ feature ê°œìˆ˜\n",
        "        self.grouped_field = grouped_field      # m: grouped field ê°œìˆ˜\n",
        "        self.feature_index = feature_index      # ì¸ì½”ë”©ëœ Xì˜ ì¹¼ëŸ¼ë“¤ì´ ë³¸ë˜ ì–´ë”” ì†Œì†ì´ì—ˆëŠ”ì§€ (ì¹¼ëŸ¼ ì¸ë±ìŠ¤)\n",
        "\n",
        "        self.fm_layer = FM_layer(num_feature, grouped_field, embedding_size, feature_index)\n",
        "        \n",
        "        self.hidden_layer1 = tf.keras.layers.Dense(units=64, activation='relu') #tf.keras.layers.Dense(ë ˆì´ì–´ ì‚¬ì´ì¦ˆ, í™œì„±í™” í•¨ìˆ˜): ì¸ê³µì‹ ê²½ë§ êµ¬ì¡°ë¥¼ êµ¬í˜„ì‹œì¼œì£¼ëŠ” í•¨ìˆ˜\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate=0.4)\n",
        "        self.hidden_layer2 = tf.keras.layers.Dense(units=32, activation='relu')\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate=0.2)\n",
        "        self.hidden_layer3 = tf.keras.layers.Dense(units=16, activation='relu')\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate=0.1)\n",
        "        self.hidden_layer4 = tf.keras.layers.Dense(units=2, activation='relu')\n",
        "\n",
        "        self.output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"DeepFM Model: # Field: {}, # Feature: {}, Embedding: {}\".format(self.grouped_field, self.num_feature, self.embedding_size)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        # 1) FM Component: (num_batch, 2)\n",
        "        y_fm, vector_inputs = self.fm_layer(inputs)\n",
        "        # print(f'Sigmoid Function shape: {y_fm.shape}') #Sigmoid Function shape: (256, 2)\n",
        "        # print(f'input vector shape: {vector_inputs.shape}') #input vector shape: (256, 101, 10)\n",
        "\n",
        "        # retrieve Dense Vectors: (num_batch, num_feature*embedding_size)\n",
        "        vector_inputs = tf.reshape(vector_inputs, [-1, self.num_feature*self.embedding_size])\n",
        "\n",
        "        # 2) Deep Component\n",
        "        y_deep = self.hidden_layer1(vector_inputs)\n",
        "        y_deep = self.dropout1(y_deep)\n",
        "        y_deep = self.hidden_layer2(y_deep)\n",
        "        y_deep = self.dropout2(y_deep)\n",
        "        y_deep = self.hidden_layer3(y_deep)\n",
        "        y_deep = self.dropout3(y_deep)\n",
        "        y_deep = self.hidden_layer4(y_deep)\n",
        "\n",
        "        # Concatenation\n",
        "        y_pred = tf.concat([y_fm, y_deep], 1)\n",
        "        y_pred = self.output_layer(y_pred)\n",
        "        y_pred = tf.reshape(y_pred, [-1, ])\n",
        "        \n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "UKpj3Xc_ETCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "6W0YrhSw8YlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_per_batch(model, x, y, opt, train_acc, train_auc):\n",
        "  with tf.GradientTape() as gt: #GradientTapes can be nested to compute higher-order derivatives. (ìë™ìœ¼ë¡œ ë¯¸ë¶„ ì‹¤í–‰.)\n",
        "      y_pred = model(x)\n",
        "      loss = tf.keras.losses.binary_crossentropy(from_logits=False, y_true=y, y_pred=y_pred)\n",
        "\n",
        "  grads = gt.gradient(target=loss, sources=model.trainable_variables)\n",
        "\n",
        "  # apply_gradients()ë¥¼ í†µí•´ processed gradientsë¥¼ ì ìš©í•¨\n",
        "  opt.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  # accuracy & auc ì„±ëŠ¥\n",
        "  train_acc.update_state(y, y_pred)\n",
        "  train_auc.update_state(y, y_pred)\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "78oFdcYCWj3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°˜ë³µ í•™ìŠµ í•¨ìˆ˜\n",
        "def training(epoch):\n",
        "    # X_train, X_test, y_train, y_test = split_data()\n",
        "    train_ds, test_ds, vec_dict, feature_index = split_data()\n",
        "    \n",
        "    model = DeepFM(embedding_size= EMBEDDING_SIZE, \n",
        "                   num_feature=len(feature_index),\n",
        "                   grouped_field=len(vec_dict), \n",
        "                   feature_index=feature_index)\n",
        "    '''\n",
        "        self.embedding_size = embedding_size    # k: ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›(í¬ê¸°)\n",
        "        self.num_feature = num_feature          # f: ì›ë˜ feature ê°œìˆ˜ (shape[1]ì— í•´ë‹¹)\n",
        "        self.grouped_field = grouped_field              # m: grouped field ê°œìˆ˜\n",
        "        self.field_index = field_index          # ì¸ì½”ë”©ëœ Xì˜ ì¹¼ëŸ¼ë“¤ì´ ë³¸ë˜ ì–´ë”” ì†Œì†ì´ì—ˆëŠ”ì§€ (ì¹¼ëŸ¼ ì¸ë±ìŠ¤)\n",
        "        \n",
        "        print('X shape: {}'.format(X_modified.shape))  \n",
        "        print('# of Feature: {}'.format(len(field_index)))\n",
        "        print('# of Field: {}'.format(len(field_dict)))\n",
        "    '''\n",
        "    # Momentum ì¥ì  + AdaGrad ì¥ì  = Adam (ëª¨ë©˜í…€ ë°©ì‹ë³´ë‹¤ ì¢Œìš° í”ë“¤ë¦¼ì´ ëœ í•¨. ì¢Œìš°í”ë“¤ë¦¼ì´ ëœ í•¨.)\n",
        "    # ì¶”ê°€ë¡œ ë…¼ë¬¸ì— ë‚˜ì˜¨ ëŒ€ë¡œ FTRL ê¸°ë²• ì‚¬ìš©\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.01) #learning_rateì€ ë³„ë„ë¡œ ì¡°ì ˆ\n",
        "    ''' \n",
        "    var1 = tf.Variable(10.0)\n",
        "    loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1\n",
        "    step_count = opt.minimize(loss, [var1]).numpy()\n",
        "    # The first step is `-learning_rate*sign(grad)`\n",
        "    var1.numpy(): 9.9\n",
        "    '''\n",
        "  \n",
        "    start = perf_counter()\n",
        "    print(\"Start Training: Batch Size: {}, Embedding Size: {}\".format(BATCH_SIZE, EMBEDDING_SIZE))\n",
        "\n",
        "    for i in range(epoch):\n",
        "      train_acc = BinaryAccuracy(threshold=0.5) #thresholdê°’ë„ ì¡°ì ˆ (0.4~0.6 ì‚¬ì´ ê°’)\n",
        "      train_auc = AUC()\n",
        "      loss_history = []\n",
        "\n",
        "      for x, y in train_ds:\n",
        "          loss = training_per_batch(model, x, y, opt, train_acc, train_auc)\n",
        "          loss_history.append(loss)\n",
        "      print(\"Epoch {}: ëˆ„ì  Loss: {:.4f}, Acc: {:.4f}, AUC: {:.4f}\".format(i+1, np.mean(loss_history), train_acc.result().numpy(), train_auc.result().numpy()))\n",
        "    \n",
        "    print(\"End of Training\")\n",
        "    test_acc = BinaryAccuracy(threshold=0.5)\n",
        "    test_auc = AUC()\n",
        "    for x, y in test_ds:\n",
        "        y_pred = model(x)\n",
        "        test_acc.update_state(y, y_pred)\n",
        "        test_auc.update_state(y, y_pred)\n",
        "    \n",
        "    print(\"í…ŒìŠ¤íŠ¸ ACC: {:.4f}, AUC: {:.4f}\".format(test_acc.result().numpy(), test_auc.result().numpy()))\n",
        "    print(\"Batch Size: {}, Embedding Size: {}\".format(BATCH_SIZE, EMBEDDING_SIZE))\n",
        "    print(f\"ê±¸ë¦° ì‹œê°„: {((perf_counter() - start)//60}ë¶„ {round((perf_counter() - start)%60, 2)}ì´ˆ\"))\n",
        "    print()"
      ],
      "metadata": {
        "id": "EaecZL4h8Xeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "í•˜ì´í¼ íŒŒë¼ë¯¸í„° ëª©ë¡:\n",
        "- batch size\n",
        "- embedding size\n",
        "- threshold\n",
        "- learning rate\n",
        "- epoch\n",
        "- activation function\n",
        "- "
      ],
      "metadata": {
        "id": "1xEqZ4l7g3R3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  training(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1yuLHEhovlL",
        "outputId": "b6c657b0-c3dd-4f21-a791-86beffc94802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Data info ----\n",
            "{'obj': ['user_gender'], 'cat': ['imp_hour', 'click_label', 'user_age_group', 'flag_used', 'category1', 'emergency_cnt'], 'cont': ['user_age', 'user_following_cnt', 'user_pay_count', 'user_parcel_post_count', 'user_transfer_count', 'user_chat_count', 'price', 'comment_cnt', 'ad_interest', 'ad_pfavcnt', 'adver_favorite_count', 'adver_grade', 'adver_item_count', 'adver_interest', 'adver_review_count', 'adver_comment_count', 'adver_pay_count', 'adver_parcel_post_count', 'adver_transfer_count', 'adver_chat_count']}\n",
            "Data Frame shape: (842463, 101)\n",
            "# of Feature: 101\n",
            "# of Field: 27\n",
            "Modified DF columns: Index(['user_gender/F', 'user_gender/M', 'imp_hour/0', 'imp_hour/1',\n",
            "       'imp_hour/2', 'imp_hour/3', 'imp_hour/4', 'imp_hour/5', 'imp_hour/6',\n",
            "       'imp_hour/7',\n",
            "       ...\n",
            "       'adver_favorite_count', 'adver_grade', 'adver_item_count',\n",
            "       'adver_interest', 'adver_review_count', 'adver_comment_count',\n",
            "       'adver_pay_count', 'adver_parcel_post_count', 'adver_transfer_count',\n",
            "       'adver_chat_count'],\n",
            "      dtype='object', length=101)\n",
            "X features' name (10): ['user_gender/F', 'user_gender/M', 'imp_hour/0', 'imp_hour/1', 'imp_hour/2', 'imp_hour/3', 'imp_hour/4', 'imp_hour/5', 'imp_hour/6', 'imp_hour/7']\n",
            "y feature's name: click_label\n",
            "\n",
            "# of train_data's rows: 589724 \n",
            "# of test_data's rows: 252739\n",
            "train:test ratio = 0.7:0.3\n",
            "Current Batch Size: 256\n",
            "train_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "test_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "Start Training: Batch Size: 256, Embedding Size: 10\n",
            "Epoch 0: ëˆ„ì  Loss: 0.1310, Acc: 0.9688, AUC: 0.6532\n",
            "Epoch 1: ëˆ„ì  Loss: 0.1269, Acc: 0.9707, AUC: 0.6744\n",
            "Epoch 2: ëˆ„ì  Loss: 0.1263, Acc: 0.9707, AUC: 0.6825\n",
            "Epoch 3: ëˆ„ì  Loss: 0.1257, Acc: 0.9707, AUC: 0.6904\n",
            "Epoch 4: ëˆ„ì  Loss: 0.1254, Acc: 0.9707, AUC: 0.6933\n",
            "Epoch 5: ëˆ„ì  Loss: 0.1253, Acc: 0.9707, AUC: 0.6950\n",
            "Epoch 6: ëˆ„ì  Loss: 0.1251, Acc: 0.9707, AUC: 0.6975\n",
            "Epoch 7: ëˆ„ì  Loss: 0.1249, Acc: 0.9707, AUC: 0.6996\n",
            "Epoch 8: ëˆ„ì  Loss: 0.1248, Acc: 0.9707, AUC: 0.7010\n",
            "Epoch 9: ëˆ„ì  Loss: 0.1246, Acc: 0.9707, AUC: 0.7029\n",
            "End of Training\n",
            "í…ŒìŠ¤íŠ¸ ACC: 0.9706, AUC: 0.6940\n",
            "Batch Size: 256, Embedding Size: 10\n",
            "ê±¸ë¦° ì‹œê°„: 753.58ì´ˆ\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SMOTE ì‚¬ìš©í•˜ì—¬ ë¶ˆê· í˜• ë°ì´í„° í™•ì¸í•˜ê¸°"
      ],
      "metadata": {
        "id": "q3apdfR8pwPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¶ˆê· í˜• ë°ì´í„° ì…‹ì¸ ë²ˆê°œì¥í„° ë°ì´í„° ì…‹ ë¶ˆê· í˜• ë¬¸ì œ ì™„í™”\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "file_name = 'final_data_v2.csv'\n",
        "file_path = os.getcwd()+'/drive/MyDrive/Colab Notebooks/' \n",
        "df = pd.read_csv(file_path+file_name, encoding='utf-8')\n",
        "df.rename(columns={'category_id_1':'category1'}, inplace=True)\n",
        "\n",
        "vec_dict, feature_index, modified_df = preprocessing()\n",
        "\n",
        "X = modified_df#.astype('float')\n",
        "y = df['click_label']\n",
        "\n",
        "# SMOTE ê¸°ë²• ì ìš© ì „\n",
        "print(y.value_counts())\n",
        "\n",
        "oversample = SMOTE(random_state=2022)\n",
        "X, y = oversample.fit_resample(X, y)\n",
        "\n",
        "# SMOTE ê¸°ë²• ì ìš© í›„\n",
        "print(y.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LlLJTzNpt5q",
        "outputId": "0b1b0dc6-fa03-4360-cc4f-abed89baa383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Data info ----\n",
            "{'obj': ['user_gender', 'user_gender'], 'cat': ['imp_hour', 'click_label', 'user_age_group', 'flag_used', 'category1', 'emergency_cnt', 'imp_hour', 'click_label', 'user_age_group', 'flag_used', 'category1', 'emergency_cnt'], 'cont': ['user_age', 'user_following_cnt', 'user_pay_count', 'user_parcel_post_count', 'user_transfer_count', 'user_chat_count', 'price', 'comment_cnt', 'ad_interest', 'ad_pfavcnt', 'adver_favorite_count', 'adver_grade', 'adver_item_count', 'adver_interest', 'adver_review_count', 'adver_comment_count', 'adver_pay_count', 'adver_parcel_post_count', 'adver_transfer_count', 'adver_chat_count', 'user_age', 'user_following_cnt', 'user_pay_count', 'user_parcel_post_count', 'user_transfer_count', 'user_chat_count', 'price', 'comment_cnt', 'ad_interest', 'ad_pfavcnt', 'adver_favorite_count', 'adver_grade', 'adver_item_count', 'adver_interest', 'adver_review_count', 'adver_comment_count', 'adver_pay_count', 'adver_parcel_post_count', 'adver_transfer_count', 'adver_chat_count']}\n",
            "Data Frame shape: (842463, 101)\n",
            "# of Feature: 101\n",
            "# of Field: 27\n",
            "Modified DF columns: Index(['user_gender/F', 'user_gender/M', 'imp_hour/0', 'imp_hour/1',\n",
            "       'imp_hour/2', 'imp_hour/3', 'imp_hour/4', 'imp_hour/5', 'imp_hour/6',\n",
            "       'imp_hour/7',\n",
            "       ...\n",
            "       'adver_favorite_count', 'adver_grade', 'adver_item_count',\n",
            "       'adver_interest', 'adver_review_count', 'adver_comment_count',\n",
            "       'adver_pay_count', 'adver_parcel_post_count', 'adver_transfer_count',\n",
            "       'adver_chat_count'],\n",
            "      dtype='object', length=101)\n",
            "0    817750\n",
            "1     24713\n",
            "Name: click_label, dtype: int64\n",
            "1    817750\n",
            "0    817750\n",
            "Name: click_label, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}