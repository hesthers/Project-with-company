{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FM model - 번개장터.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "157yvDeQI43e_PvEiUZSo9dcjPXlk9D4D",
      "authorship_tag": "ABX9TyOfmPyf//VSbouW0peBcyna"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리 및 데이터 로드"
      ],
      "metadata": {
        "id": "LTftVsyFyUdZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo3cEsKZEPZj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from itertools import repeat\n",
        "from time import perf_counter\n",
        "from tensorflow.keras.metrics import BinaryAccuracy, AUC\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
        "pd.set_option('display.max_columns', None)\n",
        "tf.keras.backend.set_floatx('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리 코드"
      ],
      "metadata": {
        "id": "EJ6nTKzXyOZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "논문 번역 발췌\n",
        "- χ\n",
        "> 유저와 아이템을 묶는 m-fields(종류)로 구성\n",
        "> 범주형 field(e.g. 성별, 위치)를 포함할 수도 있고 연속형 field(e.g. 나이)를 포함\n",
        "\n",
        "- y\n",
        "> 유저가 클릭을 했는지 안 했는지의 데이터가 0,1로 라벨링  \n",
        "\n",
        "- 각각의 범주형 field는 one-hot vector로 표현\n",
        "- 각각의 연속형 field는 값 자체로 표현되거나 이산화(discretization)한 뒤에 one-hot encdoing을 하여 표현.\n",
        "\n",
        "- 그런 다음 각각의 데이터는 ( 𝑥 ,y) shape으로 바뀌는데  𝑥  = [ $𝑥_{𝑓𝑖𝑒𝑙𝑑1} , 𝑥_{𝑓𝑖𝑒𝑙𝑑2} , ... ,  𝑥_{𝑓𝑖𝑒𝑙𝑑𝑗} ,... ,  𝑥_{𝑓𝑖𝑒𝑙𝑑𝑚}$ ] 이고 \n",
        "d차원의 벡터  \n",
        "- $𝑥_{𝑓𝑖𝑒𝑙𝑑𝑗}$  는  𝜒  의 j번째 field를 나타낸다. \n",
        "- 보통 x는 매우 차원이 높고 극도로 sparse하다."
      ],
      "metadata": {
        "id": "YGxRcGMpyDEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 256\n",
        "EMBEDDING_SIZE = 10\n",
        "\n",
        "def data_split():\n",
        "    cols = {'obj': [],\n",
        "        'cat': [],\n",
        "       'cont': []\n",
        "        }\n",
        "\n",
        "    file_name = 'final_data_v2.csv'\n",
        "    file_path = os.getcwd()+'/drive/MyDrive/Colab Notebooks/' #[:-16]는 본인 경로에 맞게 있어도 되고 없어도 됨.\n",
        "    df = pd.read_csv(file_path+file_name, encoding='utf-8')\n",
        "    df.rename(columns={'category_id_1':'category1'}, inplace=True)\n",
        "\n",
        "    # 데이터 유형별 분류하기\n",
        "    for dt_idx, dt_val in zip(df.dtypes.index, df.dtypes.values):\n",
        "        # if 'category' in dt_idx:\n",
        "        #     df[['category1']] = LabelEncoder().fit_transform(df[['category1']])\n",
        "        #     cols['cat'].append('category1')\n",
        "\n",
        "        if dt_val == 'object':\n",
        "            if ('id' in dt_idx) | ('time' in dt_idx) | ('name' in dt_idx) | ('keyword' in dt_idx) |('url' in dt_idx):\n",
        "                df.drop(columns = dt_idx, axis=1, inplace=True)\n",
        "            else:\n",
        "                cols['obj'].append(dt_idx)\n",
        "\n",
        "        else:\n",
        "            if ('id' in dt_idx) | ('time' in dt_idx):\n",
        "                df.drop(columns = dt_idx, axis=1, inplace=True)\n",
        "            else:\n",
        "                if len(df[dt_idx].value_counts()) <= 30: #연속형 데이터 중 30개 내의 범주로 나눌 수 있는 데이터 = category로 구분.\n",
        "                    cols['cat'].append(dt_idx)\n",
        "                else:\n",
        "                    if ('hour' in dt_idx) | ('group' in dt_idx):\n",
        "                        pass\n",
        "                    else:\n",
        "                        cols['cont'].append(dt_idx) \n",
        "\n",
        "    return cols"
      ],
      "metadata": {
        "id": "j_u_CUVOFSCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reorganization(df):\n",
        "  data = pd.DataFrame()\n",
        "  cols = data_split()\n",
        "  for k, v in cols.items():\n",
        "    if k == 'obj':\n",
        "      data = pd.concat([data, df[v]], axis=1)\n",
        "    elif k == 'cont':\n",
        "      data = pd.concat([data, df[v]], axis=1)\n",
        "    else:\n",
        "      data = pd.concat([data, df[v]], axis=1)\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "1CxNB2uSEt0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing():\n",
        "    file_name = 'final_data_v2.csv'\n",
        "    file_path = os.getcwd()+'/drive/MyDrive/Colab Notebooks/' \n",
        "    df = pd.read_csv(file_path+file_name, encoding='utf-8')\n",
        "    df.rename(columns={'category_id_1':'category1'}, inplace=True)\n",
        "    # 데이터 유형별 분류하기\n",
        "    data = reorganization(df)\n",
        "    # cols = data_split()\n",
        "    modified_df = pd.DataFrame()\n",
        "    vec_dict = {idx: [] for idx in range(len(data.columns))}\n",
        "    feature_index = []\n",
        "\n",
        "    for i, c in enumerate(data.columns):\n",
        "        if c in cols['obj']:\n",
        "            obj_data = pd.get_dummies(data[c], prefix=c, prefix_sep = \"/\")\n",
        "            modified_df = pd.concat([modified_df, obj_data], axis=1)\n",
        "            vec_dict[i] = list(obj_data.columns)\n",
        "            feature_index.extend(repeat(i, obj_data.shape[1]))\n",
        "\n",
        "        elif c in cols['cat']:  # click_label 컬럼 = y 변수로 사용\n",
        "            if 'click' in c:\n",
        "                pass\n",
        "            else:\n",
        "                cat_data = pd.get_dummies(data[c], prefix=c, prefix_sep = \"/\")\n",
        "                vec_dict[i] = list(cat_data.columns)\n",
        "                feature_index.extend(repeat(i, cat_data.shape[1]))\n",
        "                modified_df = pd.concat([modified_df, cat_data], axis=1)\n",
        "        else:\n",
        "            scaled_num_data = MinMaxScaler().fit_transform(df[[c]])\n",
        "            scaled_num_data = pd.DataFrame(scaled_num_data, columns = [c])\n",
        "            modified_df = pd.concat([modified_df,scaled_num_data], axis=1)\n",
        "            vec_dict[i] = list(scaled_num_data.columns)\n",
        "            feature_index.extend(repeat(i, scaled_num_data.shape[1]))\n",
        "\n",
        "    print('---- Data info ----')\n",
        "    print(cols)\n",
        "    print('Data Frame shape: {}'.format(modified_df.shape))\n",
        "    print('# of Feature: {}'.format(len(feature_index)))\n",
        "    print(f'# of Field: {len(vec_dict)}')\n",
        "    print(f'Modified DF columns: {modified_df.columns}')\n",
        "    # print(vec_dict)\n",
        "    return vec_dict, feature_index, modified_df"
      ],
      "metadata": {
        "id": "sr2X18tpasbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vec_dict, feature_index, modified_df = preprocessing()"
      ],
      "metadata": {
        "id": "oWuMedsPAlfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modified_df.head(3)"
      ],
      "metadata": {
        "id": "GSUwCGwuQvTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델링을 위한 데이터 분리하기"
      ],
      "metadata": {
        "id": "K84d3D2wzP9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 7:3으로 나누기 (혹은 8:2)\n",
        "def split_data():\n",
        "    file_name = 'final_data_v2.csv'\n",
        "    file_path = os.getcwd()+'/drive/MyDrive/Colab Notebooks/' \n",
        "    df = pd.read_csv(file_path+file_name, encoding='utf-8')\n",
        "    df.rename(columns={'category_id_1':'category1'}, inplace=True)\n",
        "\n",
        "    vec_dict, feature_index, modified_df = preprocessing()\n",
        "\n",
        "    X = modified_df#.astype('float')\n",
        "    y = df['click_label']\n",
        "\n",
        "    print(f\"X features' name (10): {X.columns.to_list()[:10]}\")\n",
        "    print(f\"y feature's name: {y.name}\")\n",
        "    print()\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 2022, stratify = y) #y 비율에 따른 층화추출 및 데이터를 7:3의 비율로 나누기\n",
        "    \n",
        "    print(f\"# of train_data's rows: {X_train.shape[0]} \\n# of test_data's rows: {X_test.shape[0]}\")\n",
        "    print(f'train:test ratio = {round(X_train.shape[0]/(X_train.shape[0]+ X_test.shape[0]),2)}:{round(X_test.shape[0]/(X_train.shape[0]+ X_test.shape[0]), 2)}')\n",
        "    # return X_train, X_test, y_train, y_test\n",
        "\n",
        "    # tf.data.Dataset.from_tensor_slices 함수: tf.data.Dataset 를 생성하는 함수로 입력된 텐서로부터 slices를 생성.\n",
        "    # shuffle 함수는 고정된 버퍼 크기로 데이터를 섞는데, 데이터가 완전히 랜덤적으로 뒤섞기 위해서는 입력된 데이터 크기보다 큰 수를 입력.\n",
        "    # tf.cast 함수: 뒤에 나온 형으로 값을 변환\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices( \n",
        "              (tf.cast(X_train.values, tf.float32), tf.cast(y_train, tf.float32))\n",
        "            ).shuffle(600000).batch(BATCH_SIZE) \n",
        "    \n",
        "    test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "              (tf.cast(X_test.values, tf.float32), tf.cast(y_test, tf.float32))\n",
        "            ).shuffle(300000).batch(BATCH_SIZE)\n",
        "    \n",
        "    print(f'Current Batch Size: {BATCH_SIZE}')\n",
        "    print(f'train_ds: {train_ds}')\n",
        "    print(f'test_ds: {test_ds}')\n",
        "    return train_ds, test_ds, vec_dict, feature_index"
      ],
      "metadata": {
        "id": "tS5mQa-Fz1ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split_data()"
      ],
      "metadata": {
        "id": "YuS4CepGQ5Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeepFM model 코드"
      ],
      "metadata": {
        "id": "dYKL_hKn6Fk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeepFM은 같은 input을 공유하는 FM component와 deep component로 구성되어 있다. \n",
        "\n",
        "$<FM component>$\n",
        "\n",
        "피쳐 i에 대해서, **스칼라 값 $w_i$는 order-1 중요도 가중치**로 사용되고 **잠재 벡터(latent vector) $V_i$는 다른 피쳐와 상호작용 정도를 측정**하는 데 사용된다. $V_i$는 order-2 상호작용을 모델링하기 위해서 FM component로 들어가는 반면 높은 차원의 피쳐 상호작용을 모델링하기 위해서 Deep Component로 들어간다. $w_i$ 와 $V_i$ 그리고 network 변수$(W^{l}, b^{l})$을 포함한 모든 변수는 아래 수식과 같이 결합된 예측 모델에서 같이 학습된다.\n",
        "\n",
        "$<FM model - based>$\n",
        "$\\hat{y}(x) := w_0 + \\sum^n_{i=1}w_i x_i + \\sum^n_{i=1}\\sum^n_{j=i+1} \\langle{v_i,v_j}\\rangle x_ix_j \\tag{1}$\n",
        "$\\langle{v_i,v_j}\\rangle := \\sum^k_{f=1}v_{i,f} \\cdot v_{j,f}\\tag{2}$\n",
        "\n",
        "$<Deep FM model>$\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAzYAAAAuCAYAAAD6IowqAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAABQVSURBVHhe7Z0HkBRVF4UfmHPOOeeIilkxJ0wlmCh1RVExJ1S0MKBVZkAMiFoqYEBQMSuKEUXEHBdzDmAAFRNq//Pd7cvfND2zM8yu0Mz5qqbY7Zne7vf6DXVPn3tvt4gKBCGEEEIIIYTIMS3jf4UQQgghhBAit0jYCCGEEEIIIXKPhI0QQgghhBAi90jYCCGEEEIIIXKPhI0QQgghhBAi90jYCCGEEEIIIXKPhI0QQgghhBAi90jYCCGEEEIIIXKPhI0QQgghhBAi90jYCCGEEEIIIXKPhI0QQgghhBAi90jYCCGEEEIIIXKPhI0QQgghhBAi90jYCCGEEEIIIXJPswqboUOHhnnnnTdsuumm4YMPPoi3CiGEEEIIIUTT0mzCBiHTrVu3sMgii4Rvv/029OzZM0yYMCF+V+SNP//8M9x6663hpptuireIWoLvcJcuXcLHH38cbxFCCCGEKE4UReHll1+2+OGXX36Jt4bw6aefhlNOOSUsv/zyZoDsuuuu4YEHHgj//vtv/ImpjzuaRdggYBAy6667bhg1alR46KGHwhtvvBFuv/12G2Se+Pvvv8P48eMnm+w88Ouvv4bff/89/q00v/32Wxg0aFA4/fTTw3bbbRfOOeec8M8//8TvNlzPCy64wObi0EMPjbeKWmLxxRcPJ554oq2DESNGxFuFEEIIIaaEeP/ee++1m+JnnnlmmGeeeWz7e++9Fzp06BDmn3/+cMMNN4TTTjstvPTSS+Hggw8Od911l30GpjbuaHJhw0DuvPPOsMQSS4RevXqFBRdcMKy11lph4MCB4bnnngvPPvts/MnpH4J7JnSZZZYJ/fv3j7dO/7zzzjthk002Cfvuu2/47rvv4q3FmXPOOUO7du1MNT/11FNhnXXWCTPNNJO9h6g5++yzwxxzzGGiZuaZZ7btovZYaqmlwhlnnBEuueQS+49JCCGEECINWoC4v1+/fhZDogXg559/Dn379jV9QFbXjjvuaP8icHB07r77bjMTnKmJO5pc2LRo0SJ07NjR7voTDDucHAPcZptt4i35AEU599xzT7ooeWC22Waz811ooYXCrLPOGm8tDQuK9MFVVlklrLHGGrbN1fabb74Z6urqakLUMAd8gV588cV4i0iy+uqrh9133z1cfPHF9h+UEEIIIUSS+vr6cPnll4fOnTub8+Jw433rrbcOrVq1irc0gAGy3nrrWaZROkOq0rijWZsH5B1cC3IAv/7669C2bdt46/TPyiuvHIYPHx4GDBgQFlhggXhraRgjAoaFtdxyy9m2zz77LPTp08fcnCWXXNK2zeiMGTPGLNOPPvoo3iKScOOCXNhPPvnE8mGFEEIIIRxqsqnHRpC0bt063trAZpttFvbZZ5/4t//DPpRFbLjhhpY9lKTSuKNZhA1qCyuJmgyHbT/99FPZdR/TC7VQYwOjR4+2OihUsy+qYcOGWaC/xRZb2MIqRtYcsY3r/ddff8Vbao+sa8A25ipvtWZJFl100bDRRhuZZcw1FkIIIYSADz/8MDz88MNhq622CvPNN1+8tTjEQ5RBrL/++uHYY4+dVAqRpJK4o0mFDYFw9+7dw9JLL20pXOTXOQ8++KClR1GnMa2DIYJu6oC23XZbK0wizeq4444zhwMmTpwYHn300XDUUUeFWWaZJXTt2jUzQB87dqxZY7SzbtOmjY0NlyPJW2+9FXr06GEKFaWKK8J+5BySKnbWWWeZUvVzouHClltuaQsjjR+P9/lbKNgnn3xyUpD8+eefh969e4e1117bHBaEShbUzVx33XX2N7AEjzzySLs+gFpGxJCa9swzz5h7Q71UGuaINDXcDa4rdUjJ4i7Og+3MHeOrFej0cfXVV9vaolDu5JNPniRuWBs777yzuWKvvPKKbcsjpDeuttpqlq6nNu5CCCGEcMgYoh7GyxpKQex7xx132Ov444+3TspZVBJ3NKmw2X777a225vzzz7ffX3jhhfDHH3/YzwTM5Mh99dVXJXPknn/+eQusK33ddttt8V9onJEjR4ZOnTqFww8/3AqYaCdHO7oVVljB3kfM7LLLLuZUAGJj9tlnt58divIRBIgRip4QGAgcWtdxPggLglcK8XfaaSdzRBB8iBA6QFCwTy3M448/biLmiiuuMBG07LLL2hxwjkkQDRRZvfvuu2Hw4MHWiIHzYwzMM7DvAQccYEKE/EV+T8N5I+ToUkEATjMHmgwwBgJuFg78+OOPlo6FQM2q02GOEGsXXnihLUaEULIxBEIPC5Jg3tdALcD1RyRfddVV9qVmXbHmgTxT1hVzQupfnmGNffPNN+G1116LtwghhBCilsEEIN4lRphrrrnirVPCDW9uqO+xxx7WDY2YF41ALIzYyaLcuKNZmgcgYjgBAji/W01Qj4DASkoWEqXBbsKVqPS10korxX+hcb788ksLxHE4mMBVV1017LnnnmHhhReOP9FwcV5//XW7644DkobW1ffdd1845JBD7H3G7SlcN954owWuXhyFQEC9Uu+Ck4HLsd9++9lYW7ZsaeKK5gq4XZ6PyHsO6pQ7/8zhpZdeavNHIT/igTmmnba3Z2ZMFGchUNL1Nd62mbZ6FMj7+TFujoeA81oa0qVwiBprQMB5+Dkzry5isBQpGsMVSudL1gI4XaT1vf/+++H777+3bQjZo48+2r7ECN48w7oA/pPJc1qdEEIIIZoGBAsxDzEljbeKQTyEqLn//vstu4hYmricOJhsoSzKjTuapcaGwTAoUs7cnSHwJlUNAcGAikFXLlq7VfrCLSkXgm7cCQQI7gj5fCeddJId2+HCoAoJQBEUSRBrKFJAqCFqgHqU5L/AuL3D1tChQ0P79u3tTj5/n4cOIUzIQaRAH+GBiOGYpHYBwuuWW24xNXvggQdOJgoptALcFU+VwyHgoiNskl3pgKIr2lYTWLsbBYyHc0bg+UL0Lmnl4IuNv+FpZ4wFAUXqlc8PxyEVj2uFgLr55pvD008/bcLsoIMOMrcDMQmM59VXXzWHg/3prpEFc4pw4prec8894Ycffigr0GZsiLv0CwcOSEXMer/cbmmIQeaFeUyuB8aPCHc3jbokzpmOgYyTLzqOHDcBWBNcs3R9F9ebLz6fZR9cMx8zn2VN8Uwi5p7vHCLVqWbfLHiAVi05ckIIIYTIhtgtGfM0BrESN9VpNkDpAjETGUml4rjG4o5mETY4AATgnKA7NrgIBHCVCJDmggCewJEgD1GEm0JqGO6JgzhAvKy55ppT5PyRlua5g17kRABI6hkOD8+QcSgWd4GAS+Ptrgn6mR/EFMVSiD3SxEg1Q5TgeAFuD4E/NRs4NEnefvtt+5daFhaHW4CQbqWHyOTBRwiXvfbaa7LWzV7vgfPiIqQSuN4IxXHjxk0SWAhGAnu6YjgILUQcc0Zgz5h4YT8iXBAx7hgxHubGnasvvvhiilodFjcpX8zjDjvsYH+HY5YzBr54OGfpF+l5QL1V1vvldktjrLhwwLwAIp/CN8StrxvWHOfsvyOUEf+IKFL9EKGs1eTYud6sI1I/YciQIZPOi7+HW8RndtttN3vgarJ4r5p9hRBCCCGaGmJS4g5i6GpvmDaLsCGow3GgZgQxg7jBHTnssMNK5txBsTvpjb0qee4IgS+1KNdee60JCVLZ0nU/7nwgbBA9Sdif2pZzzz3XxAXBPGlp1MFcdtllkz2rhzvg2GwIobq6OhMwBOKeI8g2nAqglR0iI9mZjNQy9uc8CDgdhAqqFgiGCYzdZdp8880tQE2C+/HEE0+YU+N1NOB/h30QfI6LlXLgvBB/pK4hFhENuA6MzQN2h3Q11gVjdqcHWDOo9mQg7UIPEGyIRAcnizohRB9ssMEGJZ3ANMwDdwTSL4reAGGT9T5Co1z8GrAGAPeFuedaJsHVQ/gDc4CoY41Rm8V58jCrdE4pX3r+LuNmzQwaNCh+p8Fto26L97JEXjX7puEGRrr+TAghhBC1B/Gyx7SVQty54oor2t9Ix45JGos7mkXYoLxQXUAgS9DPiZAu1BjF7qQ39irnTjp3vXFnCCS5O02qD8E33b1GjRoVf6oh8CNlCNLOh+PuziOPPGLBN6lUnAOd1JJuCMdDrOHWuHtB8M9ncSvobAYEzd4wwDuTgVt66QvpQgWngjoWQAQRqGa5TIyHoDUpmjgmwoDxp/dxsUKXL095KwaBOE0GSK1DXCE46LaWFleAEOQ8mAvGw7Wg6QA/U5OTFCcIPbYjClHw/G2HuaI9IK4Ox0mKtekF5gQ4d+afRhH777//FILBXT3GwZfawV3jd+YLoZ2ENYSLRVc95gcnyDvp8R5/M6t5BFSzr8P3GnAWyxFAQgghhJixIWYjXuWGfDJmKwdiD2JeOv8SV6YpN+5oFmHDCXnhOsEn6S60Qk4G/MUodie9sVc5d9Jxj3Bp/M4/QTRpYwSOyQtAcIdYwcXAVUBA0N44CQ7RlVdeacKCu+xZYyNo95bL7qoABeV0MuOuOAoVcIz4LC4JD9jEoeCYKF8XiQ4uy/XXX28/Uw/haWvsz1gQKQSqngIHPuYkCAq62AH7IGIQJ4Cw2XjjjW1eOF4puN7ME8emzoXjkk6VXnjJVDkECfsxF4ixLHWOCKITHIs8+SXhfKg72nvvve13gnO+SNMb7kghOHEFEb9Z5+lpj1x7T8UD/w8C0uuPfXA/WX+IZuYQRwhwY3DAfG2lqWZfh/Q61h1rWAghhBCCuM9ruF2IJCFeJ1ZMdz5jO/W/lKvw/Jssyo07mkXYgN95pnsYaVtebzAtIaAjgMOlYBKZWAQMtSW4JI4HmhTAk0LHudPeOAlBJ8E/rY4RBdTJUHjuDgsQgJNCRMDqjgLHdWcGN8hVqR+TgBI3hiCeY5IeRgDKe9TxkNZHXQmF3X369LH3ICkacAcQLd6+GjytjDobzpNAmxbZ7vYghGhZ7XfqERrUYSA8cE5KkawnwcGi3XFWJzTOnzQ0hAjjRZwdc8wxVkeTVueME5HFIvdGCggv5o80McaDo4GIwoVKO1TTA1xLzpOWhlwLalaywD3x9LxkKh7C2GvU0pC6xjwyz4gTfsYZpRsfQgWxXSzts5p9AeeT7w3XJtlwQwghhBC1DZ2CiU25IU0ck4QHdxIvYhxgGtC0iBfbiVspLcmK5yqJO5pN2HjRNw+iLBbQ/dfwPBHOhQD+1FNPtaCawB2BkJwoAnXOH3GG00Tg57iqJCjHsSGQp16HgJCuZXzWaxYIFEk5I1D0O/E4JxyT4yVb/nJM7vAjUDguFx24wFxo0sEorsfBIBCmmxgOgKfEIURIfSKdiZS/jh07TuYikRrGw0hR0Dyck+P37NnTzpd9EFDpfRB7HJNjpdV1GuYLZwkHqFgrY29FzbFp7UfaHk4bwiQNopD5I9h3kYzQQQDwZSEgZ5EDY6ikvua/gvnAuaRZwwknnFDUsfQ0M9ZJchxcc64XdygQzw5rhO5xiGWuO+/RaIB1yUOumCfmNMuqrWZfh+vCOfPdSLcUF0IIIUTtQqYJnVfJbCIVPwlZUsR/ZCkRzxIf8TgTbuQTgxQTLRXFHYVAvckpqK+of//+UeHko/Hjx8dbZwyGDBkSFYRANGzYsHhLAwVVGg0ePDgqBLNRIWCPCkFp/E6+GT58eNS6deuoICbiLVMyceLEqHv37lHXrl3t52IMGDCA/n1Rly5dooL6tm2jR4+OCgvcfk4yYsSIqCAUbf1wDuzXrl07W1MFoRWNGzcuKoibqBD0RyNHjoz3qh4/FudaLQXhFbVp06bk3E2YMCE64ogjbBwFcRFvbYD9Cl/yqCA8ojFjxsRbo2js2LFR+/bto4LIi7c0fLYg8KKCwIxatWoV1dfXx+9MTjX7gl/rzp07RwWRHm8VQgghhGiAmIXYpW/fvqYJqqHSuKPJHBusJG9Ji51ET2qK+rNSkvIMLgFpU+kaBJwT7nTjMGDD4cDMCOAc4Tacd9551qXMwcGh7qOwhiy9jfS5Uq4EdmS68xfgFKVriKAQbJvzQ0rUYostZml7OGE8AwYrkloQPpOuS6kWr/GqpPtZEhpPsD9zhVtZEC1FHSygkxxrihRAT7kD5nfgwIHmsHB3I2nNcueC9eauKLjzQiokNVq4PFlUsy/Qxpvap27dupVMVxNCCCFEbULMctFFF1k5CpqgGiqNO5pE2BB80WGJgVBfwUP/eC4JKUIzGlhnjJfnnRCUOtTjULtCty+6rc0okJJEqh2pXyxSxomF2KFDB7MMSW9D2PTo0cMESDFIwfPOX1mpZ0k8l9LTpUjRw9qk/THHJV2Lehv/e16kP62hdgvBQOezTp062XN1eGZNqbQuUs1Ir0PUeDtvxk+6IGmO1FPRiCEJn+c4yXocBCXH5TuH2Cv25a9mX/5zwiru3bt3yWsthBBCiNqGmIJYmTiGDsFTw9TEHU0ibMiNI6DnX+4IcwLF2iTnHbqbUXOCy0CrZmpX2rZta62KufNNq+MZLejjDj/1Q9Tg0M0NYYErRWE7DgU1SgiMLHDyWBP9+vULjz32mHWcQ3lTQ4OzkYTf6VxHUwOK2an34IW7g5OA+4HAIW+TZ7sAAodXYzVA/wW4LnwP6uvrLQ+0rq6uqIOFOKbJwzXXXGMdQpgbar5wORFviCG2IWq8jor5Zux8yXGsvMGFgxBETLEOEYRJqtkXcHO41r169cos7BNCCCGESMKNbGJEbqomY45ymNq4o0UhmJw8uhRCCCGEEEKInNFkNTZCCCGEEEIIMa2QsBFCCCGEEELkHgkbIYQQQgghRO6RsBFCCCGEEELkHgkbIYQQQgghRO6RsBFCCCGEEELkHgkbIYQQQgghRO6RsBFCCCGEEELkHgkbIYQQQgghRM4J4X8du7/lUFpcxQAAAABJRU5ErkJggg==)\n",
        "\n",
        "\n",
        "Parameter w의 길이는 num_feature(27)이며, Parameter V의 shape은 num_field(102), embedding_size(사용자 지정)이다. \n",
        "\n",
        "call 함수에서 보면 알 수 있듯이, 이 V행렬은 One-Hot 인코딩된 데이터에 곱해지는 구조이기 때문에 tf.nn.embedding_lookup이라는 함수를 통해 행이 복제/ 즉, 앞서 생성한 field_index의 정보를 참조하여, 같은 field에서 나온 feature일 경우, 같은 Embedding Row(V의 Row)를 공유. \n",
        "\n",
        "num_feature=len(field_index) = num_field=len(field_dict) # columns의 수"
      ],
      "metadata": {
        "id": "tL9OLxta6UbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FM_layer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_feature, grouped_field, embedding_size, feature_index):\n",
        "        super(FM_layer, self).__init__()\n",
        "        self.embedding_size = embedding_size    # k: 임베딩 벡터의 차원(크기)\n",
        "        self.num_feature = num_feature          # f: 원래 feature 개수\n",
        "        self.grouped_field = grouped_field      # m: grouped field 개수\n",
        "        self.feature_index = feature_index      # 인코딩된 X의 칼럼들이 본래 어디 소속이었는지\n",
        "\n",
        "        # Parameters of FM Layer\n",
        "        # w: capture 1st order interactions (linear - num_feature 만큼의 크기를 가진 벡터)\n",
        "        # V: capture 2nd order interactions\n",
        "        # tf.Variable: 모델링에서 weight나 bias와 같은 변수 값을 초기화하는 훈련가능한 변수\n",
        "        self.w = tf.Variable(tf.random.normal(shape=[num_feature], mean=0.0, stddev=1.0)\n",
        "                           , name='w'\n",
        "                            )\n",
        "        self.V = tf.Variable(tf.random.normal(shape=(grouped_field, embedding_size), \n",
        "                                              mean=0.0, stddev=0.01)\n",
        "                           , name='V'\n",
        "                            )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # try:\n",
        "          # print(f\"input vector's shape: {inputs.shape}\") #input vector's shape: (256, 101)\n",
        "        x_batch = tf.reshape(tf.expand_dims(inputs, axis=-1), [-1, self.num_feature, 1]) #-1: 가로 vector로 생성, self.feature_index 만큼의 행, 1열\n",
        "        # print(f\"X's Batch Shape: {x_batch.shape}\") #X's Batch Shape: (256, 101, 1)                 \n",
        "        # except ValueError as exception:\n",
        "        #   print(exception)\n",
        "        \n",
        "        # Parameter V를 feature_index에 맞게 복사하여 num_feature에 맞게 늘림 (field 수만큼 embedding)\n",
        "        embeds = tf.nn.embedding_lookup(params=self.V, ids=self.feature_index)\n",
        "        # print(f\"Embedding Layer Shape: {embeds.shape}\") #Embedding Layer Shape: (101, 10)\n",
        "        \n",
        "        # Deep Component에서 쓸 Input\n",
        "        # (batch_size, num_feature, embedding_size)\n",
        "        # order-2 layer (inner product of respective feature latent vectors)\n",
        "        vector_inputs = tf.math.multiply(x_batch, embeds) \n",
        "        # print(f'Input Layer Shape: {vector_inputs.shape}') #Input Layer Shape: (256, 101, 10)\n",
        "\n",
        "        # (batch_size, ) -> order-1 layer (linear interactions among features)\n",
        "        linear_terms = tf.reduce_sum(tf.math.multiply(self.w, inputs), axis=1, keepdims=False)\n",
        "\n",
        "        # (batch_size, ) -> order-2 features (inner product units)\n",
        "        # tf.math.pow(tf.matmul(inputs, self.V), 2) - tf.matmul(tf.math.pow(inputs, 2), tf.math.pow(self.V, 2)\n",
        "        # tf.matmul: 행렬의 곱셈\n",
        "        interactions = 0.5 * tf.subtract(\n",
        "            tf.square(tf.reduce_sum(vector_inputs, [1, 2])),\n",
        "            tf.reduce_sum(tf.square(vector_inputs), [1, 2])\n",
        "        )\n",
        "        \n",
        "        # sigmoid function for CTR prediction\n",
        "        linear_terms = tf.reshape(linear_terms, [-1, 1]) #벡터 -> tensor화된 벡터로 변환\n",
        "        interactions = tf.reshape(interactions, [-1, 1])\n",
        "        y_fm = tf.concat([linear_terms, interactions], 1)\n",
        "\n",
        "        return y_fm, vector_inputs"
      ],
      "metadata": {
        "id": "L19WQYWNES8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding Layer 구조에는 두 가지 흥미로운 점이 있다. \n",
        "\n",
        "`an embedding layer to compress the input vector to a low-dimensional,dense real-value vector before further feeding into the first hidden layer`\n",
        "\n",
        "1) **다른 field의 input vector의 길이는 다를 수 있지만 embedding은 같은 크기(k)**이다. (역자 예시: gender field는 보통 length가 남, 여 2인 반면 국적이나 나이 field의 길이는 더 길다. 하지만 embedding시에는 똑같이 k=5차원 벡터로 임베딩 된다.) \n",
        "\n",
        "\n",
        "FM component와 Deep component가 같은 feature embdding을 공유한다는 점이 주목할 만 한데 이 덕분에 두 가지 중요한 장점을 갖게 된다. \n",
        "\n",
        "1) raw feature로부터 낮은 차원과 높은 차원의 피쳐 상호작용을 둘 다 학습할 수 있다. 2) Wide & Deep 모델과 다르게 input의 직접적인 feature engineering이 필요하지 않다.\n",
        "\n",
        "2) **FM에서의 latent feature vector(V)가 이 네트워크의 가중치로 사용되고 input field vector를 압축하는 데 사용되고 학습**된다. [Zhang et al., 2016]에서는 V가 FM에 의해 pre-trained되고 이 값을 초기값으로 사용한다. 이번 논문에서는 이런 방법을 사용하는 대신 FM을 DNN과 별개로 학습 구조에 포함한다. 이렇게 함으로써 pre-training을 할 필요가 없어지고 전체 네트워크를 end-to-end로 학습할 수 있게 된다. Embedding Layer의 output을 다음과 같이 표현하면:\n",
        "\n",
        "> $a^{(0)}$ = $[e_1, e_2, ..., e_m]$\n",
        "\n",
        "$e_i$는 i번째 field의 임베딩을 나타내고 m은 field의 수를 나타낸다.\n",
        "\n",
        "$a^{(0)}$은 DNN에 들어가서 다음과 같이 forward process를 거친다:\n",
        "\n",
        "> $a^{(l+1)}$ = $\\sigma (W^{(l)}a^{(l)}+b^{(l)})$\n",
        "\n",
        "- $a^{(l)}$은 l번째 layer의 output\n",
        "- $\\sigma$: 활성화함수 & $l$: layer depth & $b^{(l)}$: l번째 layer bias\n",
        "\n",
        "the sigmoid function for CTR prediction: \n",
        "\n",
        "> $y_{DNN}$ = $\\sigma(W^{|H|+1} \\times a^H + b^{|H|+1})$ \n",
        "\n",
        ", where |H| is the number of hidden layers.\n",
        "\n",
        "출처: https://orill.tistory.com/entry/논문-리뷰-DeepFM-A-Factorization-Machine-based-Neural-Network-for-CTR-Prediction [이제 며칠 후엔]"
      ],
      "metadata": {
        "id": "bIV27sgY8jfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepFM(tf.keras.Model):\n",
        "    def __init__(self, num_feature, grouped_field, embedding_size, feature_index):\n",
        "        super(DeepFM, self).__init__()\n",
        "        self.embedding_size = embedding_size    # k: 임베딩 벡터의 차원(크기)\n",
        "        self.num_feature = num_feature          # f: 원래 feature 개수\n",
        "        self.grouped_field = grouped_field      # m: grouped field 개수\n",
        "        self.feature_index = feature_index      # 인코딩된 X의 칼럼들이 본래 어디 소속이었는지 (칼럼 인덱스)\n",
        "\n",
        "        self.fm_layer = FM_layer(num_feature, grouped_field, embedding_size, feature_index)\n",
        "        \n",
        "        self.hidden_layer1 = tf.keras.layers.Dense(units=64, activation='relu') #tf.keras.layers.Dense(레이어 사이즈, 활성화 함수): 인공신경망 구조를 구현시켜주는 함수\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate=0.4)\n",
        "        self.hidden_layer2 = tf.keras.layers.Dense(units=32, activation='relu')\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate=0.2)\n",
        "        self.hidden_layer3 = tf.keras.layers.Dense(units=16, activation='relu')\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate=0.1)\n",
        "        self.hidden_layer4 = tf.keras.layers.Dense(units=2, activation='relu')\n",
        "\n",
        "        self.output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"DeepFM Model: # Field: {}, # Feature: {}, Embedding: {}\".format(self.grouped_field, self.num_feature, self.embedding_size)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        # 1) FM Component: (num_batch, 2)\n",
        "        y_fm, vector_inputs = self.fm_layer(inputs)\n",
        "        # print(f'Sigmoid Function shape: {y_fm.shape}') #Sigmoid Function shape: (256, 2)\n",
        "        # print(f'input vector shape: {vector_inputs.shape}') #input vector shape: (256, 101, 10)\n",
        "\n",
        "        # retrieve Dense Vectors: (num_batch, num_feature*embedding_size)\n",
        "        vector_inputs = tf.reshape(vector_inputs, [-1, self.num_feature*self.embedding_size])\n",
        "\n",
        "        # 2) Deep Component\n",
        "        y_deep = self.hidden_layer1(vector_inputs)\n",
        "        y_deep = self.dropout1(y_deep)\n",
        "        y_deep = self.hidden_layer2(y_deep)\n",
        "        y_deep = self.dropout2(y_deep)\n",
        "        y_deep = self.hidden_layer3(y_deep)\n",
        "        y_deep = self.dropout3(y_deep)\n",
        "        y_deep = self.hidden_layer4(y_deep)\n",
        "\n",
        "        # Concatenation\n",
        "        y_pred = tf.concat([y_fm, y_deep], 1)\n",
        "        y_pred = self.output_layer(y_pred)\n",
        "        y_pred = tf.reshape(y_pred, [-1, ])\n",
        "        \n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "UKpj3Xc_ETCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "6W0YrhSw8YlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_per_batch(model, x, y, opt, train_acc, train_auc):\n",
        "  with tf.GradientTape() as gt: #GradientTapes can be nested to compute higher-order derivatives. (자동으로 미분 실행.)\n",
        "      y_pred = model(x)\n",
        "      loss = tf.keras.losses.binary_crossentropy(from_logits=False, y_true=y, y_pred=y_pred)\n",
        "\n",
        "  grads = gt.gradient(target=loss, sources=model.trainable_variables)\n",
        "\n",
        "  # apply_gradients()를 통해 processed gradients를 적용함\n",
        "  opt.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  # accuracy & auc 성능\n",
        "  train_acc.update_state(y, y_pred)\n",
        "  train_auc.update_state(y, y_pred)\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "78oFdcYCWj3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 반복 학습 함수\n",
        "def training(epoch):\n",
        "    # X_train, X_test, y_train, y_test = split_data()\n",
        "    train_ds, test_ds, vec_dict, feature_index = split_data()\n",
        "    \n",
        "    model = DeepFM(embedding_size= EMBEDDING_SIZE, \n",
        "                   num_feature=len(feature_index),\n",
        "                   grouped_field=len(vec_dict), \n",
        "                   feature_index=feature_index)\n",
        "    '''\n",
        "        self.embedding_size = embedding_size    # k: 임베딩 벡터의 차원(크기)\n",
        "        self.num_feature = num_feature          # f: 원래 feature 개수 (shape[1]에 해당)\n",
        "        self.grouped_field = grouped_field              # m: grouped field 개수\n",
        "        self.field_index = field_index          # 인코딩된 X의 칼럼들이 본래 어디 소속이었는지 (칼럼 인덱스)\n",
        "        \n",
        "        print('X shape: {}'.format(X_modified.shape))  \n",
        "        print('# of Feature: {}'.format(len(field_index)))\n",
        "        print('# of Field: {}'.format(len(field_dict)))\n",
        "    '''\n",
        "    # Momentum 장점 + AdaGrad 장점 = Adam (모멘텀 방식보다 좌우 흔들림이 덜 함. 좌우흔들림이 덜 함.)\n",
        "    # 추가로 논문에 나온 대로 FTRL 기법 사용\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.01) #learning_rate은 별도로 조절\n",
        "    ''' \n",
        "    var1 = tf.Variable(10.0)\n",
        "    loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1\n",
        "    step_count = opt.minimize(loss, [var1]).numpy()\n",
        "    # The first step is `-learning_rate*sign(grad)`\n",
        "    var1.numpy(): 9.9\n",
        "    '''\n",
        "  \n",
        "    start = perf_counter()\n",
        "    print(\"Start Training: Batch Size: {}, Embedding Size: {}\".format(BATCH_SIZE, EMBEDDING_SIZE))\n",
        "\n",
        "    for i in range(epoch):\n",
        "      train_acc = BinaryAccuracy(threshold=0.5) #threshold값도 조절 (0.4~0.6 사이 값)\n",
        "      train_auc = AUC()\n",
        "      loss_history = []\n",
        "\n",
        "      for x, y in train_ds:\n",
        "          loss = training_per_batch(model, x, y, opt, train_acc, train_auc)\n",
        "          loss_history.append(loss)\n",
        "      print(\"Epoch {}: 누적 Loss: {:.4f}, Acc: {:.4f}, AUC: {:.4f}\".format(i+1, np.mean(loss_history), train_acc.result().numpy(), train_auc.result().numpy()))\n",
        "    \n",
        "    print(\"End of Training\")\n",
        "    test_acc = BinaryAccuracy(threshold=0.5)\n",
        "    test_auc = AUC()\n",
        "    for x, y in test_ds:\n",
        "        y_pred = model(x)\n",
        "        test_acc.update_state(y, y_pred)\n",
        "        test_auc.update_state(y, y_pred)\n",
        "    \n",
        "    print(\"테스트 ACC: {:.4f}, AUC: {:.4f}\".format(test_acc.result().numpy(), test_auc.result().numpy()))\n",
        "    print(\"Batch Size: {}, Embedding Size: {}\".format(BATCH_SIZE, EMBEDDING_SIZE))\n",
        "    print(f\"걸린 시간: {((perf_counter() - start)//60}분 {round((perf_counter() - start)%60, 2)}초\"))\n",
        "    print()"
      ],
      "metadata": {
        "id": "EaecZL4h8Xeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "하이퍼 파라미터 목록:\n",
        "- batch size\n",
        "- embedding size\n",
        "- threshold\n",
        "- learning rate\n",
        "- epoch\n",
        "- activation function\n",
        "- "
      ],
      "metadata": {
        "id": "1xEqZ4l7g3R3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  training(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1yuLHEhovlL",
        "outputId": "b6c657b0-c3dd-4f21-a791-86beffc94802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Data info ----\n",
            "{'obj': ['user_gender'], 'cat': ['imp_hour', 'click_label', 'user_age_group', 'flag_used', 'category1', 'emergency_cnt'], 'cont': ['user_age', 'user_following_cnt', 'user_pay_count', 'user_parcel_post_count', 'user_transfer_count', 'user_chat_count', 'price', 'comment_cnt', 'ad_interest', 'ad_pfavcnt', 'adver_favorite_count', 'adver_grade', 'adver_item_count', 'adver_interest', 'adver_review_count', 'adver_comment_count', 'adver_pay_count', 'adver_parcel_post_count', 'adver_transfer_count', 'adver_chat_count']}\n",
            "Data Frame shape: (842463, 101)\n",
            "# of Feature: 101\n",
            "# of Field: 27\n",
            "Modified DF columns: Index(['user_gender/F', 'user_gender/M', 'imp_hour/0', 'imp_hour/1',\n",
            "       'imp_hour/2', 'imp_hour/3', 'imp_hour/4', 'imp_hour/5', 'imp_hour/6',\n",
            "       'imp_hour/7',\n",
            "       ...\n",
            "       'adver_favorite_count', 'adver_grade', 'adver_item_count',\n",
            "       'adver_interest', 'adver_review_count', 'adver_comment_count',\n",
            "       'adver_pay_count', 'adver_parcel_post_count', 'adver_transfer_count',\n",
            "       'adver_chat_count'],\n",
            "      dtype='object', length=101)\n",
            "X features' name (10): ['user_gender/F', 'user_gender/M', 'imp_hour/0', 'imp_hour/1', 'imp_hour/2', 'imp_hour/3', 'imp_hour/4', 'imp_hour/5', 'imp_hour/6', 'imp_hour/7']\n",
            "y feature's name: click_label\n",
            "\n",
            "# of train_data's rows: 589724 \n",
            "# of test_data's rows: 252739\n",
            "train:test ratio = 0.7:0.3\n",
            "Current Batch Size: 256\n",
            "train_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "test_ds: <BatchDataset shapes: ((None, 101), (None,)), types: (tf.float32, tf.float32)>\n",
            "Start Training: Batch Size: 256, Embedding Size: 10\n",
            "Epoch 0: 누적 Loss: 0.1310, Acc: 0.9688, AUC: 0.6532\n",
            "Epoch 1: 누적 Loss: 0.1269, Acc: 0.9707, AUC: 0.6744\n",
            "Epoch 2: 누적 Loss: 0.1263, Acc: 0.9707, AUC: 0.6825\n",
            "Epoch 3: 누적 Loss: 0.1257, Acc: 0.9707, AUC: 0.6904\n",
            "Epoch 4: 누적 Loss: 0.1254, Acc: 0.9707, AUC: 0.6933\n",
            "Epoch 5: 누적 Loss: 0.1253, Acc: 0.9707, AUC: 0.6950\n",
            "Epoch 6: 누적 Loss: 0.1251, Acc: 0.9707, AUC: 0.6975\n",
            "Epoch 7: 누적 Loss: 0.1249, Acc: 0.9707, AUC: 0.6996\n",
            "Epoch 8: 누적 Loss: 0.1248, Acc: 0.9707, AUC: 0.7010\n",
            "Epoch 9: 누적 Loss: 0.1246, Acc: 0.9707, AUC: 0.7029\n",
            "End of Training\n",
            "테스트 ACC: 0.9706, AUC: 0.6940\n",
            "Batch Size: 256, Embedding Size: 10\n",
            "걸린 시간: 753.58초\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SMOTE 사용하여 불균형 데이터 확인하기"
      ],
      "metadata": {
        "id": "q3apdfR8pwPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 불균형 데이터 셋인 번개장터 데이터 셋 불균형 문제 완화\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "file_name = 'final_data_v2.csv'\n",
        "file_path = os.getcwd()+'/drive/MyDrive/Colab Notebooks/' \n",
        "df = pd.read_csv(file_path+file_name, encoding='utf-8')\n",
        "df.rename(columns={'category_id_1':'category1'}, inplace=True)\n",
        "\n",
        "vec_dict, feature_index, modified_df = preprocessing()\n",
        "\n",
        "X = modified_df#.astype('float')\n",
        "y = df['click_label']\n",
        "\n",
        "# SMOTE 기법 적용 전\n",
        "print(y.value_counts())\n",
        "\n",
        "oversample = SMOTE(random_state=2022)\n",
        "X, y = oversample.fit_resample(X, y)\n",
        "\n",
        "# SMOTE 기법 적용 후\n",
        "print(y.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LlLJTzNpt5q",
        "outputId": "0b1b0dc6-fa03-4360-cc4f-abed89baa383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Data info ----\n",
            "{'obj': ['user_gender', 'user_gender'], 'cat': ['imp_hour', 'click_label', 'user_age_group', 'flag_used', 'category1', 'emergency_cnt', 'imp_hour', 'click_label', 'user_age_group', 'flag_used', 'category1', 'emergency_cnt'], 'cont': ['user_age', 'user_following_cnt', 'user_pay_count', 'user_parcel_post_count', 'user_transfer_count', 'user_chat_count', 'price', 'comment_cnt', 'ad_interest', 'ad_pfavcnt', 'adver_favorite_count', 'adver_grade', 'adver_item_count', 'adver_interest', 'adver_review_count', 'adver_comment_count', 'adver_pay_count', 'adver_parcel_post_count', 'adver_transfer_count', 'adver_chat_count', 'user_age', 'user_following_cnt', 'user_pay_count', 'user_parcel_post_count', 'user_transfer_count', 'user_chat_count', 'price', 'comment_cnt', 'ad_interest', 'ad_pfavcnt', 'adver_favorite_count', 'adver_grade', 'adver_item_count', 'adver_interest', 'adver_review_count', 'adver_comment_count', 'adver_pay_count', 'adver_parcel_post_count', 'adver_transfer_count', 'adver_chat_count']}\n",
            "Data Frame shape: (842463, 101)\n",
            "# of Feature: 101\n",
            "# of Field: 27\n",
            "Modified DF columns: Index(['user_gender/F', 'user_gender/M', 'imp_hour/0', 'imp_hour/1',\n",
            "       'imp_hour/2', 'imp_hour/3', 'imp_hour/4', 'imp_hour/5', 'imp_hour/6',\n",
            "       'imp_hour/7',\n",
            "       ...\n",
            "       'adver_favorite_count', 'adver_grade', 'adver_item_count',\n",
            "       'adver_interest', 'adver_review_count', 'adver_comment_count',\n",
            "       'adver_pay_count', 'adver_parcel_post_count', 'adver_transfer_count',\n",
            "       'adver_chat_count'],\n",
            "      dtype='object', length=101)\n",
            "0    817750\n",
            "1     24713\n",
            "Name: click_label, dtype: int64\n",
            "1    817750\n",
            "0    817750\n",
            "Name: click_label, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}